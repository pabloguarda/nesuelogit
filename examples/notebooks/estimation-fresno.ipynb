{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import isuelogit as isl\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/github/nesuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath('')).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pesuelogit.networks import read_OD\n",
    "from pesuelogit.etl import data_curation, add_period_id, get_tensors_by_year\n",
    "\n",
    "# Internal modules\n",
    "from nesuelogit.models import NESUELOGIT, ODParameters, UtilityParameters, BPR, MLP, KernelConstraint, \\\n",
    "    GenerationParameters, train_val_split_by_links, train_kfold, compute_generated_trips, compute_generation_factors, \\\n",
    "    create_inference_model, compute_benchmark_metrics, PolynomialLayer, bpr_function, regularization_kfold\n",
    "from nesuelogit.etl import build_network\n",
    "from nesuelogit.visualizations import  plot_predictive_performance, plot_metrics_kfold, plot_top_od_flows_periods, \\\n",
    "    plot_utility_parameters_periods, plot_rr_by_period, plot_rr_by_period_models, plot_total_trips_models, \\\n",
    "    plot_performance_functions, plot_flow_vs_traveltime, plot_flow_interaction_matrix, plot_parameters_kfold, plot_convergence_estimates, plot_parameters\n",
    "from nesuelogit.metrics import mse, btcg_mse, mnrmse, mape, nrmse, r2_score, zscore, z2score\n",
    "from nesuelogit.utils import read_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report global runtime\n",
    "t0_global = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read nodes and link-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(isl.config.dirs['read_network_data'] + 'nodes/fresno-nodes-gis-data.csv')\n",
    "\n",
    "links_df = pd.read_csv(isl.config.dirs['read_network_data'] + 'links/' 'fresno-link-specific-data.csv',\n",
    "                       converters={\"link_key\": ast.literal_eval, \"pems_id\": ast.literal_eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_network(links_df=links_df, nodes_df=nodes_df, crs='epsg:4326', key= 'fresno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.0[s] with sparse format\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n"
     ]
    }
   ],
   "source": [
    "read_OD(network=network, sparse=True)\n",
    "\n",
    "q_historic = np.repeat(network.q.flatten()[np.newaxis, :], 6, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths were read and incidence matrix were built\n"
     ]
    }
   ],
   "source": [
    "#read_paths(network=network, update_incidence_matrices=True, filename='paths-fresno.csv')\n",
    "read_paths(network=network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')\n",
    "\n",
    "# For quick testing\n",
    "# Q = network.load_OD(sparsify_OD(network.Q, prop_od_pairs=0.99))\n",
    "# load_k_shortest_paths(network=network, k=2, update_incidence_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = isl.config.dirs['read_network_data'] + 'links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "# Select data from Tuesday to Thursday\n",
    "df = df[df['date'].dt.dayofweek.between(1, 3)]\n",
    "# df = df[df['date'].dt.year == 2019]\n",
    "# Select data from Wednesday only\n",
    "#df = df[df['date'].dt.dayofweek.between(2, 2)]\n",
    "\n",
    "# Select data from one day only\n",
    "#df = df[df['date'] == \"2019-10-02\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add period id for timevarying estimation\n",
    "\n",
    "period_feature = 'hour'\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df[period_feature].astype(str)\n",
    "# df['period'] = df.period.map(hash)\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units in miles per hour\n",
    "df[['speed_ref_avg','speed_hist_avg','speed_max']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)\n",
    "\n",
    "df['tt_sd'] = df['tt_sd_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['speed_ref_avg','speed_hist_avg', 'tt_ff', 'tt_avg','tt_sd_adj']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_df['population'] = nodes_df['pop_tract']#/nodes_df['nodes_tract']\n",
    "#TODO: Compute area of each census tract and normalize number of stops for that\n",
    "# nodes_df['bus_stops'] = nodes_df['stops_tract']/nodes_df['pop_tract']\n",
    "nodes_df['bus_stops'] = nodes_df['stops_tract']#/nodes_df['nodes_tract']\n",
    "nodes_df['income'] = nodes_df['median_inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_generation = ['population','income', 'bus_stops']\n",
    "\n",
    "nodes_df = nodes_df[['key','type'] + features_generation]\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(nodes_df[features_generation])\n",
    "nodes_df[features_generation] = imp_mean.transform(nodes_df[features_generation])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(nodes_df[features_generation].values)\n",
    "# scaler = preprocessing.MinMaxScaler().fit(nodes_df[features_generation].values)\n",
    "nodes_df[features_generation] = scaler.transform(nodes_df[features_generation].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURES_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "# _FEATURES_Z = ['tt_sd']\n",
    "# _FEATURES_Z = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year\n",
    "X, Y = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2019')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2020')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in network.links:\n",
    "    network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==str(link.key)].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This correlation should be positive\n",
    "df[['counts','tt_avg']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[_FEATURES_Z].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DTYPE = tf.float32\n",
    "\n",
    "# Data between 4pm and 5pm to estimate LUE, ODLUE and ODLULPE models\n",
    "X, Y = get_tensors_by_year(df[df.hour.isin([16])], features_Z = _FEATURES_Z, network = network)\n",
    "\n",
    "# Hourly data DURING morning and afternoon peak hour windows (6 hour intervals) to estimate TVODLULPE\n",
    "XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8, 15,16,17])], features_Z = _FEATURES_Z, network = network)\n",
    "\n",
    "# Split in training and test sets\n",
    "X_train, X_val, Y_train, Y_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [X[2019], X[2020], Y[2019], Y[2020]])\n",
    "XT_train, XT_val, YT_train, YT_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [XT[2019], XT[2020], YT[2019], YT[2020]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "XT[2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSS_WEIGHTS ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
    "_EQUILIBRIUM_STAGE = True\n",
    "_ALTERNATING_OPTIMIZATION = False\n",
    "\n",
    "_LR = {'learning': 1e-1, 'equilibrium': 1e-2}\n",
    "\n",
    "_BATCH_SIZE = 1 #16\n",
    "_EPOCHS = {'learning': 30, 'equilibrium': 30}\n",
    "# _EPOCHS = {'learning': 1, 'equilibrium': 1}\n",
    "# _XTICKS_SPACING = 2\n",
    "_XTICKS_SPACING = 5\n",
    "_EPOCHS_PRINT_INTERVAL = {'learning':1, 'equilibrium':1}\n",
    "\n",
    "_RELATIVE_GAP = 5e-2\n",
    "# _RELATIVE_GAP = 1e-1\n",
    "\n",
    "# _LOSS_METRIC  = mnrmse\n",
    "# _LOSS_METRIC  = nrmse\n",
    "# _LOSS_METRIC  = zscore\n",
    "_LOSS_METRIC  = z2score\n",
    "\n",
    "# With GPU\n",
    "_OPTIMIZERS = {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "              'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "val_results_dfs = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(network, homogenous = False, diagonal = False, adjacency_constraint = True, poly_order = 4,\n",
    "               alpha_prior = 0.15, beta_prior = 4, pretrain = False, dtype =_DTYPE, link_specific = True):\n",
    "\n",
    "    return MLP(n_links=len(network.links),\n",
    "               free_flow_traveltimes=[link.bpr.tf for link in network.links],\n",
    "               capacities=[link.bpr.k for link in network.links],\n",
    "               kernel_constraint=KernelConstraint(\n",
    "                   link_keys=[(link.key[0], link.key[1]) for link in network.links],\n",
    "                   dtype=dtype,\n",
    "                   capacities=[link.bpr.k for link in network.links],\n",
    "                   adjacency_constraint=adjacency_constraint,\n",
    "                   free_flow_traveltimes=[link.bpr.tf for link in network.links],\n",
    "                   diagonal= diagonal,\n",
    "                   homogenous=homogenous,\n",
    "                   bounds_clipping = [0,10],\n",
    "                   min_diagonal_value = 1e-1\n",
    "               ),\n",
    "               trainable =True,\n",
    "               polynomial_layer= PolynomialLayer(poly_order=poly_order,\n",
    "                                                 trainable = True,\n",
    "                                                 pretrain_weights=pretrain,\n",
    "                                                 alpha_prior = alpha_prior, beta_prior=beta_prior,\n",
    "                                                 kernel_constraint=tf.keras.constraints.NonNeg(),\n",
    "                                                 link_specific = link_specific\n",
    "                                                 ),\n",
    "               # alpha_relu = 1e-1,\n",
    "               alpha_relu = 0,\n",
    "               depth=1,\n",
    "               max_traveltime_factor = None,\n",
    "               dtype=dtype)\n",
    "\n",
    "def create_bpr(network, alpha_prior = 1, beta_prior = 1, dtype =_DTYPE):\n",
    "    return BPR(keys=['alpha', 'beta'],\n",
    "               # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "               # initial_values={'alpha': alpha_prior, 'beta': beta_prior}, # Consistent with MLP initialization\n",
    "               initial_values={'alpha': alpha_prior * tf.ones(len(network.links), dtype = dtype),\n",
    "                               'beta': beta_prior * tf.ones(len(network.links), dtype = dtype)},\n",
    "               true_values={'alpha': 0.15, 'beta': 4},\n",
    "               trainables={'alpha': True, 'beta':True},\n",
    "               capacities = [link.bpr.k for link in network.links],\n",
    "               free_flow_traveltimes =[link.bpr.tf for link in network.links],\n",
    "               dtype = dtype\n",
    "               )\n",
    "\n",
    "def create_model(network, model_key = 'tvgodlulpe', dtype=_DTYPE, n_periods=1, features_Z=_FEATURES_Z, historic_g=None,\n",
    "                                      performance_function=None, utility_parameters = None, od_parameters = None,\n",
    "                                      generation_parameters = None, generation = True, od_trainable = False,\n",
    "                                      utility_trainable = True, pretrain_generation_weights = True, generation_trainable = True):\n",
    "\n",
    "    if utility_parameters is None:\n",
    "        utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                               features_Z=features_Z,\n",
    "                                               # initial_values={\n",
    "                                               #                 'tt': -10,\n",
    "                                               #                 'tt_sd': -10, 'median_inc': 1,\n",
    "                                               #                 'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                               #                 'psc_factor': 0,\n",
    "                                               #                 'fixed_effect': np.zeros_like(network.links)},\n",
    "                                               initial_values={\n",
    "                                                   'tt': -3.0597,\n",
    "                                                   'tt_sd': -3.2678, 'median_inc': 0,\n",
    "                                                   'incidents': -4.5368, 'bus_stops': 0, 'intersections': -3.8788,\n",
    "                                                   'psc_factor': 0,\n",
    "                                                   'fixed_effect': np.zeros_like(network.links)},\n",
    "\n",
    "                                               signs={'tt': '-', 'tt_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                      'bus_stops': '-', 'intersections': '-'},\n",
    "                                               trainables={'psc_factor': False, 'fixed_effect': utility_trainable,\n",
    "                                                           'tt': utility_trainable, 'tt_sd': True, 'median_inc': True,\n",
    "                                                           'incidents': True,\n",
    "                                                           'bus_stops': True, 'intersections': True\n",
    "                                                           },\n",
    "                                               time_varying=True,\n",
    "                                               dtype=dtype\n",
    "                                               )\n",
    "\n",
    "    if performance_function is None:\n",
    "        # performance_function = create_bpr(network = network, dtype = dtype)\n",
    "        performance_function = create_mlp(network = network, dtype = dtype)\n",
    "\n",
    "    if generation_parameters is None and generation:\n",
    "        generation_parameters = GenerationParameters(\n",
    "            features_Z=['population', 'income', 'bus_stops'],\n",
    "            keys=['fixed_effect_od', 'fixed_effect_origin', 'fixed_effect_destination'],\n",
    "            initial_values={'income': 0, 'population': 0, 'bus_stops': 0,\n",
    "                            # 'fixed_effect': historic_g[0]\n",
    "                            'fixed_effect': historic_g\n",
    "                            },\n",
    "            signs={'income': '+', 'population': '+', 'bus_stops': '-'},\n",
    "            trainables={'fixed_effect': generation_trainable,\n",
    "                        'income': False, 'population': False, 'bus_stops': False,\n",
    "                        # 'income': True, 'population': True, 'bus_stops': True,\n",
    "                        'fixed_effect_origin': False, 'fixed_effect_destination': False, 'fixed_effect_od': generation_trainable\n",
    "                        # 'fixed_effect_origin': False, 'fixed_effect_destination': generation_trainable, 'fixed_effect_od': False\n",
    "                        },\n",
    "            # trainables={'fixed_effect': True, 'income': True, 'population': True, 'bus_stops': True},\n",
    "            # trainables={'fixed_effect': False, 'income': False, 'population': False, 'bus_stops': False},\n",
    "            time_varying=True,\n",
    "            # historic_g = od_parameters.compute_generated_trips(),\n",
    "            historic_g= historic_g,\n",
    "            pretrain_generation_weights=pretrain_generation_weights,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    if od_parameters is None:\n",
    "        od_parameters = ODParameters(key='od',\n",
    "                                     initial_values= q_historic,\n",
    "                                     historic_values={10: network.q.flatten()},\n",
    "                                     # total_trips={0: 1e5, 1: 1e5, 2: 1e5, 9: 1e5, 10: 1e5, 11: 1e5},\n",
    "                                     ods=network.ods,\n",
    "                                     n_periods=n_periods,\n",
    "                                     time_varying=True,\n",
    "                                     trainable= od_trainable,\n",
    "                                     )\n",
    "\n",
    "    model = NESUELOGIT(\n",
    "        key=model_key,\n",
    "        network=network,\n",
    "        dtype=dtype,\n",
    "        utility=utility_parameters,\n",
    "        performance_function=performance_function,\n",
    "        od=od_parameters,\n",
    "        generation=generation_parameters,\n",
    "        n_periods=n_periods\n",
    "    )\n",
    "\n",
    "    return model, {'utility_parameters': utility_parameters, 'generation_parameters': generation_parameters,\n",
    "                   'od_parameters': od_parameters, 'performance_function': performance_function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Total trips tvodlulpe pesuelogit:\n",
    "# Epoch 0: 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04\n",
    "# Final epoch: 6.4e+04 6.6e+04 6.3e+04 7.8e+04 7.9e+04 7.9e+04\n",
    "# Growth factor captures the difference between the reference OD at epoch 0 and the estimated OD.\n",
    "growth_factor = 7.9/6.6\n",
    "#growth_factor = 1\n",
    "\n",
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                              flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(q = q_historic, ods= network.ods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Benchmark (TVODLULPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "models['tvodlulpe'], _ = create_model(\n",
    "    model_key = 'tvodlulpe',\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE, alpha_prior = 0.9327, beta_prior = 4.1017),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "models['tvodlulpe'].key = 'tvodlulpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_dfs['tvodlulpe'], val_results_dfs['tvodlulpe'] = models['tvodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    node_data=nodes_df,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    loss_weights=_LOSS_WEIGHTS,\n",
    "    loss_metric= _LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvodlulpe'].save_weights(models['tvodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['tvodlulpe'].build()\n",
    "# models['tvodlulpe'].load_weights(\"output/models/230623133023_tvgodlulpe_fresno.h5\")\n",
    "# models['tvodlulpe'].update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flow_range = range(0,6000,100)\n",
    "sharey = False\n",
    "\n",
    "fig, axs = plot_performance_functions(model = models['tvodlulpe'],\n",
    "                                      network = network,\n",
    "                                      marginal = False,\n",
    "                                      #alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                                      #beta=models['tvodlulpe'].performance_function.beta,\n",
    "                                      alpha=0.15*np.ones(network.get_n_links()),\n",
    "                                      beta=4*np.ones(network.get_n_links()),\n",
    "                                      sharey = sharey,\n",
    "                                      flow_range = flow_range\n",
    "                                      # selected_links = np.random.choice(range(tntp_network.get_n_links()), 10, replace=False)\n",
    "                                      )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_xlim(xmin=-10)\n",
    "    ax.set_ylim(ymin=-1)\n",
    "    ax.legend(loc='upper left', title = 'period')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-comparison-all-link-performance-functions-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "selected_links = np.random.choice(range(network.get_n_links()), 10, replace=False)\n",
    "\n",
    "plot_performance_functions(model=models['tvodlulpe'],\n",
    "                           network=network,\n",
    "                           marginal=True,\n",
    "                           # alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           # beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           alpha=0.15*np.ones(network.get_n_links()),\n",
    "                           beta=4*np.ones(network.get_n_links()),\n",
    "                           selected_links = selected_links,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = models['tvodlulpe'],\n",
    "                        observed_traveltime=models['tvodlulpe'].mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= models['tvodlulpe'].mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            # curves=['travel time', 'link flow'],\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage = _EPOCHS['learning']\n",
    "                            )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-mse-tvodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['equilibrium'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'])\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-mape-tvodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            curves=['equilibrium'],\n",
    "                            yaxis_label='mape (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_convergence_estimates(\n",
    "    estimates=train_results_dfs['tvodlulpe'].assign(\n",
    "        relative_gap=np.abs(train_results_dfs['tvodlulpe']['relative_gap']))[['epoch', 'relative_gap']],\n",
    "    xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "#ax.set_ylim(ymin=1e-2, ymax = 5.5e-2)\n",
    "#ax.set_ylim(ymin=1e-2)\n",
    "ax.set_xlim(xmin=-0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('relative gap (log scale)')\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-gap-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_convergence_estimates(estimates=train_results_dfs['tvodlulpe'][['epoch', 'alpha', 'beta']],\n",
    "                           xticks_spacing=_XTICKS_SPACING)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#sns.displot(plot_data, x=\"value\", hue=\"parameter\", multiple=\"stack\", kind=\"hist\", alpha=0.8, norm_hist=True)\n",
    "\n",
    "plot_data = pd.melt(pd.DataFrame({'alpha': models['tvodlulpe'].performance_function.alpha,\n",
    "                              'beta': models['tvodlulpe'].performance_function.beta}), var_name='parameter')\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "# sns.histplot(plot_data, x=\"value\", stat='density', hue=\"parameter\", alpha =0.8, ax = ax, binwidth = 0.5)\n",
    "\n",
    "sns.displot(plot_data, x=\"value\", kind=\"hist\", stat='percent', common_norm=False, hue = \"parameter\", alpha=0.8, facet_kws=dict(sharey=False, sharex = False), binwidth = 0.5)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-distribution-link-performance-parameters-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "theta_df = plot_utility_parameters_periods(models['tvodlulpe'], period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "print(theta_df)\n",
    "#print(theta_df.values)\n",
    "\n",
    "plot_rr_by_period(models['tvodlulpe'],period_keys, model_key = 'tvodlulpe', period_feature='hour')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "sns.histplot(pd.DataFrame({'fixed_effect': np.array(models['tvodlulpe'].fixed_effect)}), stat='density', x=\"fixed_effect\", alpha=0.8, ax = ax, binwidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "theta_df = plot_utility_parameters_periods(models['tvodlulpe'], period_keys=period_keys, period_feature='hour', plot = False).reset_index().\\\n",
    "    drop(['hour','index'], axis = 1).reset_index().rename(columns = {'index':'period'})\n",
    "\n",
    "# theta_df = pd.melt(theta_df.assign(rr = theta_df.apply(compute_rr, axis=1)), id_vars = 'period', var_name = 'parameter').assign(group = 'utility').\\\n",
    "#     replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 'rr': 'reliability ratio', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income'})\n",
    "\n",
    "theta_df = pd.melt(theta_df, id_vars = 'period', var_name = 'parameter').assign(group = 'utility').\\\n",
    "    replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 'rr': 'reliability ratio', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income'})\n",
    "\n",
    "theta_df['hour'] = theta_df['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models['tvodlulpe'].period_dict.items()})\n",
    "\n",
    "fig, axs = plot_parameters(df = theta_df, n_cols_legend = 3, figsize = (6,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-utility-periods-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap with flows of top od pairs\n",
    "plot_top_od_flows_periods(models['tvodlulpe'],\n",
    "                          historic_od= network.q.flatten(),\n",
    "                          period_keys = period_keys,\n",
    "                          period_feature='hour', top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"theta = \"\n",
    "      f\"{dict(zip(models['tvodlulpe'].utility.true_values.keys(), list(np.mean(models['tvodlulpe'].theta.numpy(), axis=0))))}\")\n",
    "\n",
    "print(f\"alpha = {np.mean(models['tvodlulpe'].performance_function.alpha): 0.2f}, \"\n",
    "          f\"beta  = {np.mean(models['tvodlulpe'].performance_function.beta): 0.2f}\")\n",
    "\n",
    "print(f\"Avg abs diff of observed and estimated OD: \"\n",
    "      f\"{np.mean(np.abs(models['tvodlulpe'].q - network.q.flatten())): 0.2f}\")\n",
    "\n",
    "print(f\"Avg observed OD: {np.mean(np.abs(network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_df = models['tvodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                      X=XT_train, Y=YT_train).assign(dataset='training')\n",
    "metrics_df = pd.concat([metrics_df,\n",
    "                        models['tvodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                                 X=XT_val, Y=YT_val).assign(dataset='validation'),\n",
    "                        compute_benchmark_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score}, Y_ref=YT_train,\n",
    "                                                  Y=YT_val).assign(\n",
    "                            dataset='benchmark')\n",
    "                        ])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntvgodlulpe: Time specific utility and generation, and link specific parameters for performance functions')\n",
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "def create_tvgodlulpe_model():\n",
    "    return create_model(\n",
    "        n_periods = n_periods,\n",
    "        network = network,\n",
    "        performance_function = create_mlp(network = network,\n",
    "                                          poly_order = 4, pretrain = False, link_specific = False),\n",
    "        historic_g= generated_trips,\n",
    "        generation = True,\n",
    "        generation_trainable = True,\n",
    "        utility_trainable = True)\n",
    "\n",
    "models['tvgodlulpe'], _ = create_tvgodlulpe_model()\n",
    "models['tvgodlulpe'].key = 'tvgodlulpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# May remove validation set for faster training.\n",
    "train_results_dfs['tvgodlulpe'], val_results_dfs['tvgodlulpe'] = models['tvgodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # batch_size=None,\n",
    "    loss_weights= _LOSS_WEIGHTS,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvgodlulpe'].save_weights(models['tvgodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvgodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['tvgodlulpe'].build()\n",
    "# models['tvgodlulpe'].load_weights(\"output/models/230623142010_tvgodlulpe_fresno.h5\")\n",
    "# models['tvgodlulpe'].update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_links = np.random.choice(range(network.get_n_links()), 20, replace=False)\n",
    "\n",
    "#weights_interaction = models['tvgodlulpe'].performance_function.weights[1].numpy()\n",
    "weights_interaction = models['tvgodlulpe'].performance_function.model.layers[0].kernel_matrix.numpy()\n",
    "initial_weights_interaction = models['tvgodlulpe'].performance_function.kernel_constraint.initial_values.numpy()\n",
    "# Plot sign of values in flow interaction matrix\n",
    "plot_flow_interaction_matrix(\n",
    "    flow_interaction_matrix=weights_interaction,\n",
    "    masking_matrix = initial_weights_interaction,\n",
    "    # flow_interaction_matrix=np.where(weights_interaction>0, 1, -1)*np.where(weights_interaction==0, 0, 1),\n",
    "    # masking_matrix = np.where(initial_weights_interaction>0, 1, -1)*np.where(initial_weights_interaction==0, 0, 1),\n",
    "    links_ids = selected_links + 1,\n",
    "    # vmin = -0.05, vmax = 0.05\n",
    "    vmin = -1, vmax = 1\n",
    ")\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kernel-link-performance-functions-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Final weights interaction:\\n {weights_interaction}')\n",
    "print(f'\\nInitial weights interaction:\\n',initial_weights_interaction)\n",
    "\n",
    "# non_diagonal_mlp_weights = weights[~np.eye(weights.shape[0], dtype=bool)]\n",
    "# print(f\"Percentage of non-diagonal terms that are non-positive: {len(non_diagonal_mlp_weights[(non_diagonal_mlp_weights<=0)])/len(non_diagonal_mlp_weights):.2%}\")\n",
    "\n",
    "# non_zero_elements = weights_interaction[np.where(models['tvgodlulpe'].performance_function.kernel_constraint.initial_values.numpy()>0)]\n",
    "# print(f\"Percentage of non-zero terms that are positive: {len(non_zero_elements[(non_zero_elements>0)])/len(non_zero_elements):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of weight in interaction flow matrix. Elements that are set to zero by default are excluded from non-diagonal\n",
    "plot_data = pd.concat([pd.DataFrame({'value': tf.linalg.diag_part(weights_interaction).numpy().flatten(), 'parameter': 'diagonal'}),\n",
    "                                  pd.DataFrame({'value': weights_interaction[np.where(~np.eye(weights_interaction.shape[0],dtype=bool)*models['tvgodlulpe'].performance_function.kernel_constraint.adjacency_constraint_matrix)], 'parameter': 'non-diagonal'})])\n",
    "\n",
    "sns.displot(data = plot_data, x=\"value\", kind=\"hist\", stat='percent', common_norm=False, hue = \"parameter\", alpha=0.8,\n",
    "            facet_kws=dict(sharey=False, sharex = False), binwidth = 0.5) # , col=\"parameter\"\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-distribution-link-performance-parameters-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_data['value_abs'] = plot_data.value.abs()\n",
    "\n",
    "print(plot_data.groupby('parameter')[['value', 'value_abs']].mean())\n",
    "print(plot_data.groupby('parameter')[['value']].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of polynomial\n",
    "print(f\"polynomial weights {tf.reduce_mean(models['tvgodlulpe'].performance_function.polynomial_layer.poly_weights,0).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flow_range = range(0,6000,100)\n",
    "sharey = False\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = False,\n",
    "                           # alpha=models['tvgodlulpe'].performance_function.polynomial_layer.alpha_prior,\n",
    "                           # beta=models['tvgodlulpe'].performance_function.polynomial_layer.beta_prior,\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-comparison-all-link-performance-functions-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "selected_links = np.random.choice(range(network.get_n_links()), 10, replace=False)\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = False,\n",
    "                           selected_links = selected_links,\n",
    "                           # alpha=models['tvgodlulpe'].performance_function.polynomial_layer.alpha_prior,\n",
    "                           # beta=models['tvgodlulpe'].performance_function.polynomial_layer.beta_prior,\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot_performance_functions(model = models['tvgodlulpe'],\n",
    "#                                network = network,\n",
    "#                                marginal = False,\n",
    "#                                selected_links = selected_links,\n",
    "#                                alpha = 0.15,\n",
    "#                                beta = 4,\n",
    "#                                palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "#                                sharey = sharey,\n",
    "#                                flow_range = flow_range\n",
    "#                                )\n",
    "# plt.show()\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = True,\n",
    "                           selected_links = selected_links,\n",
    "                           # alpha = 0.15*np.ones(network.get_n_links()),\n",
    "                           # beta = 4*np.ones(network.get_n_links()),\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = models['tvgodlulpe'],\n",
    "                        observed_traveltime=models['tvgodlulpe'].mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= models['tvgodlulpe'].mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            # curves=['travel time', 'link flow'],\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage = _EPOCHS['learning']\n",
    "                            )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-mse-tvgodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['equilibrium'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'])\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-mape-tvgodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            curves=['equilibrium'],\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_convergence_estimates(\n",
    "    estimates=train_results_dfs['tvgodlulpe'].assign(\n",
    "        relative_gap=np.abs(train_results_dfs['tvgodlulpe']['relative_gap']))[['epoch', 'relative_gap']],\n",
    "    xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "#ax.set_ylim(ymin=1e-2)\n",
    "# ax.set_ylim(ymin=1e-2, ymax = 5.5e-2)\n",
    "ax.set_xlim(xmin=-0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('relative gap (log scale)')\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-gap-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "theta_df = plot_utility_parameters_periods(models['tvgodlulpe'], period_keys = period_keys, period_feature='hour')\n",
    "#print(theta_df.values)\n",
    "\n",
    "plot_rr_by_period(models['tvgodlulpe'], period_keys, model_key = 'tvgodlulpe', period_feature='hour')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "sns.histplot(pd.DataFrame({'fixed_effect': np.array(models['tvodlulpe'].fixed_effect)}), stat='density', x=\"fixed_effect\", alpha=0.8, ax = ax, binwidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot parameters by period\n",
    "theta_df = plot_utility_parameters_periods(models['tvgodlulpe'], period_keys=period_keys, period_feature='hour', plot = False).reset_index().\\\n",
    "    drop(['hour','index'], axis = 1).reset_index().rename(columns = {'index':'period'})\n",
    "\n",
    "# theta_df = pd.melt(theta_df.assign(rr = theta_df.apply(compute_rr, axis=1)), id_vars = 'period', var_name = 'parameter').assign(group = 'utility')\n",
    "\n",
    "theta_df = pd.melt(theta_df, id_vars = 'period', var_name = 'parameter').assign(group = 'utility')\n",
    "\n",
    "kappa_df = pd.concat([pd.DataFrame({'period': [period], 'parameter': [feature], 'value': [models['tvgodlulpe'].kappa[period, i].numpy()]}) for period in range(models['tvgodlulpe'].kappa.shape[0]) for i, feature in enumerate(models['tvgodlulpe'].generation.features)]).assign(group = 'generation')\n",
    "\n",
    "parameters_df = pd.concat([theta_df, kappa_df]).\\\n",
    "    replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income', 'rr': 'reliability ratio'})\n",
    "\n",
    "parameters_df['hour'] = parameters_df['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models['tvgodlulpe'].period_dict.items()})\n",
    "\n",
    "fig, ax = plot_parameters(df = parameters_df[parameters_df.group == 'utility'], n_cols_legend = 3, figsize = (6,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-utility-periods-tvgodlulpe.png')\n",
    "\n",
    "plot_parameters(df = parameters_df[parameters_df.group == 'generation'], n_cols_legend = 3, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-generation-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Theta:', models['tvgodlulpe'].theta.numpy().T)\n",
    "print('Kappa:', models['tvgodlulpe'].kappa.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap with flows of top od pairs\n",
    "plot_top_od_flows_periods(models['tvgodlulpe'],\n",
    "                          historic_od= network.q.flatten(),\n",
    "                          period_keys = period_keys,\n",
    "                          period_feature='hour', top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"theta = \"\n",
    "      f\"{dict(zip(models['tvgodlulpe'].utility.true_values.keys(), list(np.mean(models['tvgodlulpe'].theta.numpy(), axis=0))))}\")\n",
    "print(f\"kappa= \"\n",
    "      f\"{dict(zip(models['tvgodlulpe'].generation.features, list(np.mean(models['tvgodlulpe'].kappa.numpy(), axis=0))))}\")\n",
    "\n",
    "if models['tvgodlulpe'].performance_function.type == 'bpr':\n",
    "    print(f\"alpha = {np.mean(models['tvgodlulpe'].performance_function.alpha): 0.2f}, \"\n",
    "          f\"beta  = {np.mean(models['tvgodlulpe'].performance_function.beta): 0.2f}\")\n",
    "\n",
    "print(f\"Avg abs diff of observed and estimated OD: \"\n",
    "      f\"{np.mean(np.abs(models['tvgodlulpe'].q - network.q.flatten())): 0.2f}\")\n",
    "\n",
    "print(f\"Avg observed OD: {np.mean(np.abs(network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = models['tvgodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                      X=XT_train, Y=YT_train).assign(dataset='training')\n",
    "# metrics_df = pd.concat([metrics_df,\n",
    "#                         models['tvgodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "#                                                                  X=XT_val, Y=YT_val).assign(dataset='validation'),\n",
    "#                         compute_benchmark_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score}, Y_ref=YT_train,\n",
    "#                                                   Y=YT_val).assign(\n",
    "#                             dataset='benchmark')\n",
    "#                         ])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Optimal equilibrium hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_RELATIVE_GAP_HP = 0\n",
    "_EPOCHS_HP = {'learning': 30, 'equilibrium': 0}\n",
    "_N_SPLITS_HP = 3\n",
    "\n",
    "target_metric = 'mse'\n",
    "target_component = 'flow'\n",
    "\n",
    "# grid_equilibrium = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "grid_equilibrium = [1e-1, 5e-1, 1, 2]\n",
    "loss_weights = []\n",
    "for i in grid_equilibrium:\n",
    "    loss_weights.append(_LOSS_WEIGHTS.copy())\n",
    "    loss_weights[-1]['equilibrium'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = compute_generated_trips(q=tf.stack(q_historic), ods=network.ods)\n",
    "\n",
    "model, _ = create_model(\n",
    "    n_periods = n_periods,\n",
    "    network = network,\n",
    "    performance_function = create_mlp(network = network,\n",
    "                                      poly_order = 4, pretrain = False, link_specific = False),\n",
    "    historic_g= generated_trips,\n",
    "    generation = True,\n",
    "    generation_trainable = True,\n",
    "    utility_trainable = True)\n",
    "model.key = 'tvgodlulpe'\n",
    "\n",
    "regularization_metrics_df, optimal_weights, optimal_metrics_kfold_df, optimal_parameters_kfold_df \\\n",
    "    = regularization_kfold(\n",
    "    loss_weights=loss_weights,\n",
    "    target_metric = 'mse',\n",
    "    target_component = 'flow',\n",
    "    n_splits=_N_SPLITS_HP,\n",
    "    random_state=_SEED,\n",
    "    model=model,\n",
    "    X=XT_train, Y=YT_train,\n",
    "    optimizers=_OPTIMIZERS,\n",
    "    # generalization_error={'train': False, 'validation': True},\n",
    "    node_data=nodes_df,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows=True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP_HP,\n",
    "    # batch_size=1,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5},\n",
    "    epochs=_EPOCHS_HP,\n",
    "    # epochs_print_interval = _EPOCHS,\n",
    "    # epochs_print_interval = {'learning': 30, 'equilibrium': 30},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "filepath = f\"output/tables/{ts}_regularization_sensitivity_{'fresno'}.csv\"\n",
    "regularization_metrics_df.to_csv(filepath, index=False)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "regularization_plot_df = pd.read_csv(filepath)\n",
    "regularization_plot_df = regularization_plot_df.sort_values(by = ['component', 'lambda_equilibrium', 'dataset'])\n",
    "regularization_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in validation set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'validation') & (regularization_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'validation') & (regularization_plot_df.component == 'flow') ]['value'])\n",
    "z = regularization_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = regularization_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-20, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in training set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'training') & (regularization_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'training') & (regularization_plot_df.component == 'flow') ]['value'])\n",
    "z = regularization_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = regularization_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-25, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "print(generation_factors)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "growth_factor = 7.9/6.6\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(q = network.q.flatten()[np.newaxis,:], ods= network.ods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TVODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['reference-model'], _ = create_model(\n",
    "#     n_periods=n_periods, network=network, historic_g=generation_factors.values[:, np.newaxis] * generated_trips)\n",
    "#\n",
    "# train_results_dfs['reference-model'], val_results_dfs['reference-model'] = models['reference-model'].fit(\n",
    "#     XT_train, YT_train, XT_val, YT_val,\n",
    "#     node_data=nodes_df,\n",
    "#     optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "#                  'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "#                  },\n",
    "#     batch_size=_BATCH_SIZE,\n",
    "#     # loss_weights= _LOSS_WEIGHTS,\n",
    "#     loss_weights = {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1},\n",
    "#     loss_metric=_LOSS_METRIC,\n",
    "#     equilibrium_stage= True,\n",
    "#     alternating_optimization= False,\n",
    "#     pretrain_link_flows = True,\n",
    "#     threshold_relative_gap= 2e-1,\n",
    "#     epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "#     epochs=_EPOCHS)\n",
    "\n",
    "reference_model = models['tvodlulpe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model, _ = create_model(\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE, alpha_prior = 0.9327, beta_prior = 4.1017),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inference_model.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "inference_model.load_weights(reference_model._filepath_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Model estimated with all data from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= 1,\n",
    "                        # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                        # batch_size= None,\n",
    "                        optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        # threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                        threshold_relative_gap=_RELATIVE_GAP,\n",
    "                        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                        epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TVGODLULPE is retrained with optimal hyperparameter weight for equilibrium\n",
    "\n",
    "models['tvgodlulpe-optimal'], _ = create_tvgodlulpe_model()\n",
    "#optimal_weights ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_dfs['tvgodlulpe-optimal'], val_results_dfs['tvgodlulpe-optimal'] = models['tvgodlulpe-optimal'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # batch_size=None,\n",
    "    loss_weights= optimal_weights,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP_HP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS_HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['reference-model'], _ = create_model(\n",
    "#     n_periods=n_periods, network=network, historic_g=generation_factors.values[:, np.newaxis] * generated_trips)\n",
    "#\n",
    "# train_results_dfs['reference-model'], val_results_dfs['reference-model'] = models['reference-model'].fit(\n",
    "#     XT_train, YT_train, XT_val, YT_val,\n",
    "#     node_data=nodes_df,\n",
    "#     optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "#                  'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "#                  },\n",
    "#     batch_size=_BATCH_SIZE,\n",
    "#     # loss_weights= _LOSS_WEIGHTS,\n",
    "#     loss_weights = {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1},\n",
    "#     loss_metric=_LOSS_METRIC,\n",
    "#     equilibrium_stage= True,\n",
    "#     alternating_optimization= False,\n",
    "#     pretrain_link_flows = True,\n",
    "#     threshold_relative_gap= 2e-1,\n",
    "#     epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "#     epochs=_EPOCHS)\n",
    "\n",
    "reference_model = models['tvgodlulpe-optimal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model, _ = create_tvgodlulpe_model()\n",
    "# inference_model, _ = create_model(\n",
    "#     n_periods = n_periods,\n",
    "#     network = network,\n",
    "#     performance_function = create_mlp(network = network,\n",
    "#                                       poly_order = 4, alpha_prior = 1, beta_prior = 4, pretrain = False, link_specific = False),\n",
    "#     historic_g= generated_trips,\n",
    "#     generation = True,\n",
    "#     utility_trainable = True)\n",
    "\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inference_model.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "inference_model.load_weights(reference_model._filepath_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Model estimated with all data from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= 1,\n",
    "                        # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                        # batch_size= None,\n",
    "                        optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        # threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                        threshold_relative_gap=_RELATIVE_GAP,\n",
    "                        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                        epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to mask observed travel time and observed flow\n",
    "\n",
    "predictions = pd.DataFrame({'link_key': list(network.links_keys) * Y_train.shape[0],\n",
    "                            'observed_traveltime': Y_train[:, :, 0].numpy().flatten(),\n",
    "                            'observed_flow': Y_train[:, :, 1].numpy().flatten()})\n",
    "\n",
    "predictions['date'] = sorted(df[df.hour == 16].loc[df[df.hour == 16].year == 2019, 'date'])\n",
    "\n",
    "# # TODO: Write predictions for TVODLULPE model\n",
    "# for model in [lue,odlue,odlulpe]:\n",
    "#\n",
    "#     predicted_flows = model.flows()\n",
    "#     predicted_traveltimes = model.traveltimes()\n",
    "#\n",
    "#     predictions['predicted_traveltime_' + model.key] = np.tile(predicted_traveltimes, (Y_train.shape[0], 1)).flatten()\n",
    "#     predictions['predicted_flow_' + model.key] = np.tile(predicted_flows, (Y_train.shape[0], 1)).flatten()\n",
    "#\n",
    "# predictions.to_csv(f\"./output/tables/{datetime.now().strftime('%y%m%d%H%M%S')}_train_predictions_{network.key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold = {}\n",
    "metrics_kfold_df = {}\n",
    "parameters_kfold_df = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - TVODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold['tvodlulpe'], _ = create_model(\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 # historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "models_kfold['tvodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "#models_kfold['tvodlulpe'].generation._pretrain_generation_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Use pretrained weights\n",
    "models_kfold['tvodlulpe'].load_weights(models['tvodlulpe']._filepath_weights)\n",
    "# model_kfold.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "# model_kfold.load_weights(\"output/models/230616191846_tvgodlulpe_fresno.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "# q_reference = models['tvgodlulpe'].q\n",
    "q_reference = models_kfold['tvodlulpe'].q\n",
    "\n",
    "q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvodlulpe'].od.historic_values = q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization.\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'], parameters_kfold_df['tvodlulpe'] = train_kfold(\n",
    "    n_splits=10,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows = False,\n",
    "    # threshold_relative_gap= 5e-2,\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=1,\n",
    "    epochs = _EPOCHS,\n",
    "    # epochs={'learning': 30, 'equilibrium': 10},\n",
    "    # batch_size=None,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5}\n",
    "    # epochs_print_interval= {'learning': 100, 'equilibrium': 100},\n",
    "    # epochs= {'learning': 4, 'equilibrium': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'].to_csv(f\"./output/experiments/{datetime.now().strftime('%y%m%d%H%M%S')}_kfold_{network.key}.csv\")\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])], metric_name = 'mape', showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-mape-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvodlulpe']['hour'] = parameters_kfold_df['tvodlulpe'].period.replace({v: k for k, v in models_kfold['tvodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['parameter'] = parameters_kfold_df['tvodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe'] = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold['tvgodlulpe'], _ = create_model(\n",
    "    n_periods = n_periods,\n",
    "    network = network,\n",
    "    performance_function = create_mlp(network = network,\n",
    "                                      poly_order = 4, pretrain = False, link_specific = False),\n",
    "    historic_g= generated_trips,\n",
    "    generation = True,\n",
    "    generation_trainable = True,\n",
    "    utility_trainable = True)\n",
    "\n",
    "models_kfold['tvgodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "models_kfold['tvgodlulpe'].generation._pretrain_generation_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Use pretrained weights\n",
    "models_kfold['tvgodlulpe'].load_weights(models['tvgodlulpe']._filepath_weights)\n",
    "# model_kfold.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "# model_kfold.load_weights(\"output/models/230616191846_tvgodlulpe_fresno.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "# q_reference = models['tvgodlulpe'].q\n",
    "q_reference = models_kfold['tvgodlulpe'].q\n",
    "\n",
    "q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvgodlulpe'].od.historic_values = q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization.\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvgodlulpe'], parameters_kfold_df['tvgodlulpe'] = train_kfold(\n",
    "    n_splits=10,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvgodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows = False,\n",
    "    # threshold_relative_gap= 5e-2,\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=1,\n",
    "    epochs = _EPOCHS,\n",
    "    # epochs={'learning': 30, 'equilibrium': 10},\n",
    "    # batch_size=None,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5}\n",
    "    # epochs_print_interval= {'learning': 100, 'equilibrium': 100},\n",
    "    # epochs= {'learning': 4, 'equilibrium': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvgodlulpe'].to_csv(f\"./output/experiments/{datetime.now().strftime('%y%m%d%H%M%S')}_kfold_{network.key}.csv\")\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])], metric_name = 'mape', showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-mape-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvgodlulpe']['hour'] = parameters_kfold_df['tvgodlulpe'].period.replace({v: k for k, v in models_kfold['tvgodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['parameter'] = parameters_kfold_df['tvgodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvgodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvgodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe'] = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'generation'], n_cols_legend = 3, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-generation-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write estimation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_df, val_results_df \\\n",
    "    = map(lambda x: pd.concat([results.assign(model = model)[['model'] + list(results.columns)]\n",
    "                               for model, results in x.items()],axis = 0), [train_results_dfs, val_results_dfs])\n",
    "\n",
    "train_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_train_results_{network.key}.csv\"\n",
    "val_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_validation_results_{network.key}.csv\"\n",
    "train_results_df.to_csv(f\"./output/tables/{train_filename}\")\n",
    "print(f'File {train_filename} was written')\n",
    "val_results_df.to_csv(f\"./output/tables/{val_filename}\")\n",
    "print(f'File {val_filename} was written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of parameter estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'parameter': [], 'model': []})\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    model_results = {**dict(zip(['tt'] + _FEATURES_Z, list(np.mean(model.theta.numpy(), axis=0)))),\n",
    "                     **{'rr': float(model.get_parameters_estimates().eval('tt_sd/tt').iloc[0]),\n",
    "                        'fixed_effect_mean': np.mean(model.fixed_effect),\n",
    "                        'fixed_effect_std': np.std(model.fixed_effect),\n",
    "                        'od_mean': np.mean(model.q),\n",
    "                        'od_std': np.std(model.q)\n",
    "                        }}\n",
    "\n",
    "    if model.performance_function.type == 'bpr':\n",
    "        model_results = {**model_results, **{'alpha_mean': np.mean(model.performance_function.alpha),\n",
    "                                             'alpha_std': np.std(model.performance_function.alpha),\n",
    "                                             'beta_mean': np.mean(model.performance_function.beta),\n",
    "                                             'beta_std': np.std(model.performance_function.beta)}}\n",
    "\n",
    "    model_results = pd.DataFrame({'parameter': model_results.keys(), 'values': model_results.values()}). \\\n",
    "        assign(model=model_key)\n",
    "\n",
    "    results = pd.concat([results, model_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.pivot_table(index = ['parameter'], columns = 'model', values = 'values', sort=False).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Stats by period\n",
    "model = models['tvgodlulpe']\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('theta:', pd.DataFrame(model.theta.numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('kappa:', pd.DataFrame(model.kappa.numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('OD matrix', pd.DataFrame({'od_mean': np.mean(model.q,axis = 1), 'od_std': np.std(model.q, axis = 1), 'od_total':np.sum(model.q,axis = 1)}).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of models goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_losses = pd.DataFrame({})\n",
    "loss_columns = ['loss_flow', 'loss_traveltime', 'loss_equilibrium', 'loss_total']\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    results_losses_model = model.split_results(train_results_dfs[model_key])[1].assign(model=model_key)\n",
    "    results_losses_model = results_losses_model[results_losses_model.epoch == _EPOCHS['learning']].iloc[[0]]\n",
    "    results_losses = pd.concat([results_losses, results_losses_model])\n",
    "\n",
    "results_losses[loss_columns] = (results_losses[loss_columns] - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print(results_losses[['model'] + list(results_losses.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of convergence toward true rr across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimates = {}\n",
    "train_losses = {}\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    train_estimates[model_key], train_losses[model_key] = model.split_results(results=train_results_dfs[model_key])\n",
    "\n",
    "    train_estimates[model_key]['model'] = model_key\n",
    "\n",
    "train_estimates_df = pd.concat(train_estimates.values())\n",
    "\n",
    "train_estimates_df['rr'] = train_estimates_df['tt_sd'] / train_estimates_df['tt']\n",
    "\n",
    "estimates = train_estimates_df[['epoch', 'model', 'rr']].reset_index().drop('index', axis=1)\n",
    "#estimates = estimates[estimates.epoch != 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "g = sns.lineplot(data=estimates, x='epoch', hue='model', y='rr')\n",
    "\n",
    "# ax.hlines(y=compute_rr(models.popitem().true_values), xmin=estimates['epoch'].min(), xmax=estimates['epoch'].max(), linestyle='--', label = 'truth')\n",
    "\n",
    "ax.set_ylabel('reliability ratio')\n",
    "\n",
    "ax.set_xticks(np.arange(estimates['epoch'].min(), estimates['epoch'].max() + 2, _XTICKS_SPACING))\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "#plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of reliability ratio by hour for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_ratios = plot_rr_by_period_models(models, period_keys, period_feature='hour', join = True)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-reliability-ratios-periods.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(reliability_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print(reliability_ratios[reliability_ratios.model == 'tvgodlulpe'][['hour','rr']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reliability_ratios.groupby('model')[['rr']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of total trips by hour for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips = plot_total_trips_models(models = models, period_feature = 'hour', period_keys = period_keys,\n",
    "                                      historic_od = network.q.flatten(), join = True)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-total-trips-periods.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(total_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips.groupby('model')[['total_trips']].mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips.groupby('model')[['total_trips']].sum().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Global runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'runtime: {time.time()-t0_global:0.1f} [s]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nesuelogit",
   "language": "python",
   "name": "nesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
