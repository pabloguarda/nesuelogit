{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import isuelogit as isl\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/github/nesuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath('')).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "sys.path.append(os.path.join(main_dir, 'src'))\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pesuelogit.networks import read_OD\n",
    "from pesuelogit.etl import data_curation, add_period_id, get_tensors_by_year\n",
    "\n",
    "# Internal modules\n",
    "from nesuelogit.models import NESUELOGIT, ODParameters, UtilityParameters, BPR, MLP, KernelConstraint, \\\n",
    "    GenerationParameters, train_val_split_by_links, train_kfold, compute_generated_trips, compute_generation_factors, \\\n",
    "    create_inference_model, compute_benchmark_metrics, PolynomialLayer, bpr_function, regularization_kfold\n",
    "from nesuelogit.etl import build_network\n",
    "from nesuelogit.visualizations import  plot_predictive_performance, plot_metrics_kfold, plot_top_od_flows_periods, \\\n",
    "    plot_utility_parameters_periods, plot_rr_by_period, plot_rr_by_period_models, plot_total_trips_models, \\\n",
    "    plot_performance_functions, plot_flow_vs_traveltime, plot_flow_interaction_matrix, plot_parameters_kfold, plot_convergence_estimates, plot_parameters\n",
    "from nesuelogit.metrics import mse, btcg_mse, mnrmse, mape, nrmse, r2_score, zscore, z2score\n",
    "from nesuelogit.utils import read_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report global runtime\n",
    "t0_global = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read nodes and link-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(isl.config.dirs['read_network_data'] + 'nodes/fresno-nodes-gis-data.csv')\n",
    "\n",
    "links_df = pd.read_csv(isl.config.dirs['read_network_data'] + 'links/' 'fresno-link-specific-data.csv',\n",
    "                       converters={\"link_key\": ast.literal_eval, \"pems_id\": ast.literal_eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_network(links_df=links_df, nodes_df=nodes_df, crs='epsg:4326', key= 'fresno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.0[s] with sparse format\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n"
     ]
    }
   ],
   "source": [
    "read_OD(network=network, sparse=True)\n",
    "\n",
    "q_historic = np.repeat(network.q.flatten()[np.newaxis, :], 6, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths were read and incidence matrix were built\n"
     ]
    }
   ],
   "source": [
    "#read_paths(network=network, update_incidence_matrices=True, filename='paths-fresno.csv')\n",
    "read_paths(network=network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')\n",
    "\n",
    "# For quick testing\n",
    "# Q = network.load_OD(sparsify_OD(network.Q, prop_od_pairs=0.99))\n",
    "# load_k_shortest_paths(network=network, k=2, update_incidence_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = isl.config.dirs['read_network_data'] + 'links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "# Select data from Tuesday to Thursday\n",
    "df = df[df['date'].dt.dayofweek.between(1, 3)]\n",
    "# df = df[df['date'].dt.year == 2019]\n",
    "# Select data from Wednesday only\n",
    "#df = df[df['date'].dt.dayofweek.between(2, 2)]\n",
    "\n",
    "# Select data from one day only\n",
    "#df = df[df['date'] == \"2019-10-02\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hour  period_id\n",
      "11     6          0\n",
      "12     7          1\n",
      "13     8          2\n",
      "14     9          3\n",
      "0     10          4\n",
      "1     11          5\n",
      "2     12          6\n",
      "3     13          7\n",
      "4     14          8\n",
      "5     15          9\n",
      "6     16         10\n",
      "7     17         11\n",
      "8     18         12\n",
      "9     19         13\n",
      "10    20         14\n"
     ]
    }
   ],
   "source": [
    "# Add period id for timevarying estimation\n",
    "\n",
    "period_feature = 'hour'\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df[period_feature].astype(str)\n",
    "# df['period'] = df.period.map(hash)\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            link_key  observed  counts  capacity [veh]  tt_ff [min]  \\\n0     (0, 1621, '0')         0     NaN          1800.0        0.098   \n1  (1239, 1630, '0')         0     NaN             inf        0.000   \n2    (228, 192, '0')         1  1667.0          2400.0        0.110   \n3  (1243, 1631, '0')         0     NaN             inf        0.000   \n4  (1244, 1632, '0')         0     NaN             inf        0.000   \n\n   speed_ff[mi/hr]      inrix_id  pems_ids link_type   id  ...  median_age  \\\n0               45  1.626616e+09        []     LWRLK    1  ...        31.1   \n1            99999           NaN        []     DMDLK  565  ...         0.0   \n2               65  1.626659e+09  [602350]     LWRLK  564  ...        16.6   \n3            99999           NaN        []     DMDLK  563  ...         0.0   \n4            99999           NaN        []     DMDLK  562  ...         0.0   \n\n   incidents  bus_stops  intersections       date  hour  tf_inrix  year  \\\n0          0          0              1 2019-10-01    10     0.221  2019   \n1          0          0              0 2019-10-01    10     0.000  2019   \n2          0          0              0 2019-10-01    10     0.112  2019   \n3          0          0              0 2019-10-01    10     0.000  2019   \n4          0          0              0 2019-10-01    10     0.000  2019   \n\n          period  period_id  \n0  2019-10-01-10          4  \n1  2019-10-01-10          4  \n2  2019-10-01-10          4  \n3  2019-10-01-10          4  \n4  2019-10-01-10          4  \n\n[5 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link_key</th>\n      <th>observed</th>\n      <th>counts</th>\n      <th>capacity [veh]</th>\n      <th>tt_ff [min]</th>\n      <th>speed_ff[mi/hr]</th>\n      <th>inrix_id</th>\n      <th>pems_ids</th>\n      <th>link_type</th>\n      <th>id</th>\n      <th>...</th>\n      <th>median_age</th>\n      <th>incidents</th>\n      <th>bus_stops</th>\n      <th>intersections</th>\n      <th>date</th>\n      <th>hour</th>\n      <th>tf_inrix</th>\n      <th>year</th>\n      <th>period</th>\n      <th>period_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(0, 1621, '0')</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1800.0</td>\n      <td>0.098</td>\n      <td>45</td>\n      <td>1.626616e+09</td>\n      <td>[]</td>\n      <td>LWRLK</td>\n      <td>1</td>\n      <td>...</td>\n      <td>31.1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2019-10-01</td>\n      <td>10</td>\n      <td>0.221</td>\n      <td>2019</td>\n      <td>2019-10-01-10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(1239, 1630, '0')</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>inf</td>\n      <td>0.000</td>\n      <td>99999</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>DMDLK</td>\n      <td>565</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2019-10-01</td>\n      <td>10</td>\n      <td>0.000</td>\n      <td>2019</td>\n      <td>2019-10-01-10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(228, 192, '0')</td>\n      <td>1</td>\n      <td>1667.0</td>\n      <td>2400.0</td>\n      <td>0.110</td>\n      <td>65</td>\n      <td>1.626659e+09</td>\n      <td>[602350]</td>\n      <td>LWRLK</td>\n      <td>564</td>\n      <td>...</td>\n      <td>16.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2019-10-01</td>\n      <td>10</td>\n      <td>0.112</td>\n      <td>2019</td>\n      <td>2019-10-01-10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(1243, 1631, '0')</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>inf</td>\n      <td>0.000</td>\n      <td>99999</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>DMDLK</td>\n      <td>563</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2019-10-01</td>\n      <td>10</td>\n      <td>0.000</td>\n      <td>2019</td>\n      <td>2019-10-01-10</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(1244, 1632, '0')</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>inf</td>\n      <td>0.000</td>\n      <td>99999</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>DMDLK</td>\n      <td>562</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2019-10-01</td>\n      <td>10</td>\n      <td>0.000</td>\n      <td>2019</td>\n      <td>2019-10-01-10</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 43 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       speed_ref_avg  speed_hist_avg     speed_max\ncount   1.013460e+06    1.009583e+06  1.013460e+06\nmean    1.924470e+01    1.767589e+01  2.051278e+01\nstd     1.982261e+01    1.909664e+01  2.140424e+01\nmin     0.000000e+00    0.000000e+00  0.000000e+00\n25%     0.000000e+00    0.000000e+00  0.000000e+00\n50%     2.112700e+01    1.755400e+01  2.112700e+01\n75%     2.796200e+01    2.547600e+01  3.106900e+01\nmax     6.772900e+01    7.891400e+01  8.388500e+01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speed_ref_avg</th>\n      <th>speed_hist_avg</th>\n      <th>speed_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.013460e+06</td>\n      <td>1.009583e+06</td>\n      <td>1.013460e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.924470e+01</td>\n      <td>1.767589e+01</td>\n      <td>2.051278e+01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.982261e+01</td>\n      <td>1.909664e+01</td>\n      <td>2.140424e+01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.112700e+01</td>\n      <td>1.755400e+01</td>\n      <td>2.112700e+01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.796200e+01</td>\n      <td>2.547600e+01</td>\n      <td>3.106900e+01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>6.772900e+01</td>\n      <td>7.891400e+01</td>\n      <td>8.388500e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Units in miles per hour\n",
    "df[['speed_ref_avg','speed_hist_avg','speed_max']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['link_key', 'observed', 'counts', 'capacity [veh]', 'tt_ff [min]',\n       'speed_ff[mi/hr]', 'inrix_id', 'pems_ids', 'link_type', 'id', 'rhoj',\n       'lane', 'ff_speed', 'length', 'alpha', 'beta', 'tf', 'k', 'inrix_id.1',\n       'speed_avg', 'speed_ref_avg', 'speed_hist_avg', 'tt_avg', 'speed_max',\n       'speed_sd', 'speed_cv', 'speed_hist_sd', 'speed_ref_sd', 'tt_sd',\n       'tt_var', 'tt_cv', 'road_closures', 'median_inc', 'median_age',\n       'incidents', 'bus_stops', 'intersections', 'date', 'hour', 'tf_inrix',\n       'year', 'period', 'period_id'],\n      dtype='object')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)\n",
    "\n",
    "df['tt_sd'] = df['tt_sd_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       speed_ref_avg  speed_hist_avg         tt_ff         tt_avg  \\\ncount   1.013460e+06    1.009583e+06  1.013460e+06  887813.000000   \nmean    1.924470e+01    1.767589e+01  2.038423e-01       0.219493   \nstd     1.982261e+01    1.909664e+01  2.610045e-01       0.262030   \nmin     0.000000e+00    0.000000e+00  0.000000e+00       0.000000   \n25%     0.000000e+00    0.000000e+00  0.000000e+00       0.000000   \n50%     2.112700e+01    1.755400e+01  1.497376e-01       0.167050   \n75%     2.796200e+01    2.547600e+01  2.792397e-01       0.312986   \nmax     6.772900e+01    7.891400e+01  4.220601e+00       3.827439   \n\n          tt_sd_adj  \ncount  1.013460e+06  \nmean   3.022799e-04  \nstd    4.638521e-04  \nmin    0.000000e+00  \n25%    0.000000e+00  \n50%    1.856268e-04  \n75%    4.248116e-04  \nmax    1.243267e-02  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speed_ref_avg</th>\n      <th>speed_hist_avg</th>\n      <th>tt_ff</th>\n      <th>tt_avg</th>\n      <th>tt_sd_adj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.013460e+06</td>\n      <td>1.009583e+06</td>\n      <td>1.013460e+06</td>\n      <td>887813.000000</td>\n      <td>1.013460e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.924470e+01</td>\n      <td>1.767589e+01</td>\n      <td>2.038423e-01</td>\n      <td>0.219493</td>\n      <td>3.022799e-04</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.982261e+01</td>\n      <td>1.909664e+01</td>\n      <td>2.610045e-01</td>\n      <td>0.262030</td>\n      <td>4.638521e-04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.112700e+01</td>\n      <td>1.755400e+01</td>\n      <td>1.497376e-01</td>\n      <td>0.167050</td>\n      <td>1.856268e-04</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.796200e+01</td>\n      <td>2.547600e+01</td>\n      <td>2.792397e-01</td>\n      <td>0.312986</td>\n      <td>4.248116e-04</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>6.772900e+01</td>\n      <td>7.891400e+01</td>\n      <td>4.220601e+00</td>\n      <td>3.827439</td>\n      <td>1.243267e-02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['speed_ref_avg','speed_hist_avg', 'tt_ff', 'tt_avg','tt_sd_adj']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_df['population'] = nodes_df['pop_tract']#/nodes_df['nodes_tract']\n",
    "#TODO: Compute area of each census tract and normalize number of stops for that\n",
    "# nodes_df['bus_stops'] = nodes_df['stops_tract']/nodes_df['pop_tract']\n",
    "nodes_df['bus_stops'] = nodes_df['stops_tract']#/nodes_df['nodes_tract']\n",
    "nodes_df['income'] = nodes_df['median_inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_generation = ['population','income', 'bus_stops']\n",
    "\n",
    "nodes_df = nodes_df[['key','type'] + features_generation]\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(nodes_df[features_generation])\n",
    "nodes_df[features_generation] = imp_mean.transform(nodes_df[features_generation])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(nodes_df[features_generation].values)\n",
    "# scaler = preprocessing.MinMaxScaler().fit(nodes_df[features_generation].values)\n",
    "nodes_df[features_generation] = scaler.transform(nodes_df[features_generation].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURES_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "# _FEATURES_Z = ['tt_sd']\n",
    "# _FEATURES_Z = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year\n",
    "X, Y = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "             counts          tt_ff         tt_avg       tf_inrix  \\\ncount  31624.000000  542925.000000  468303.000000  542925.000000   \nmean    1761.140959       0.204903       0.221931       0.200169   \nstd      770.228931       0.262226       0.266852       0.228208   \nmin        1.000000       0.000000       0.000000       0.000000   \n25%     1249.000000       0.000000       0.000000       0.000000   \n50%     1686.000000       0.150455       0.168820       0.152000   \n75%     2162.450000       0.282111       0.318261       0.280000   \nmax     4807.000000       4.220601       3.827439       2.302000   \n\n               tt_sd  \ncount  542925.000000  \nmean        0.018137  \nstd         0.027831  \nmin         0.000000  \n25%         0.000000  \n50%         0.011138  \n75%         0.025489  \nmax         0.745960  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n      <th>tt_ff</th>\n      <th>tt_avg</th>\n      <th>tf_inrix</th>\n      <th>tt_sd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>31624.000000</td>\n      <td>542925.000000</td>\n      <td>468303.000000</td>\n      <td>542925.000000</td>\n      <td>542925.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1761.140959</td>\n      <td>0.204903</td>\n      <td>0.221931</td>\n      <td>0.200169</td>\n      <td>0.018137</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>770.228931</td>\n      <td>0.262226</td>\n      <td>0.266852</td>\n      <td>0.228208</td>\n      <td>0.027831</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1249.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1686.000000</td>\n      <td>0.150455</td>\n      <td>0.168820</td>\n      <td>0.152000</td>\n      <td>0.011138</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2162.450000</td>\n      <td>0.282111</td>\n      <td>0.318261</td>\n      <td>0.280000</td>\n      <td>0.025489</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4807.000000</td>\n      <td>4.220601</td>\n      <td>3.827439</td>\n      <td>2.302000</td>\n      <td>0.745960</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('year == 2019')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "             counts          tt_ff         tt_avg       tf_inrix  \\\ncount  27472.000000  470535.000000  419510.000000  470535.000000   \nmean    1605.127777       0.202618       0.216770       0.198494   \nstd      743.713869       0.259583       0.256512       0.226003   \nmin        6.000000       0.000000       0.000000       0.000000   \n25%     1108.450000       0.000000       0.000000       0.000000   \n50%     1528.900000       0.149437       0.165341       0.150000   \n75%     1978.000000       0.274658       0.306717       0.273000   \nmax     4766.000000       4.220601       3.274444       2.113000   \n\n               tt_sd  \ncount  470535.000000  \nmean        0.018137  \nstd         0.027831  \nmin         0.000000  \n25%         0.000000  \n50%         0.011138  \n75%         0.025489  \nmax         0.745960  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n      <th>tt_ff</th>\n      <th>tt_avg</th>\n      <th>tf_inrix</th>\n      <th>tt_sd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>27472.000000</td>\n      <td>470535.000000</td>\n      <td>419510.000000</td>\n      <td>470535.000000</td>\n      <td>470535.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1605.127777</td>\n      <td>0.202618</td>\n      <td>0.216770</td>\n      <td>0.198494</td>\n      <td>0.018137</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>743.713869</td>\n      <td>0.259583</td>\n      <td>0.256512</td>\n      <td>0.226003</td>\n      <td>0.027831</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1108.450000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1528.900000</td>\n      <td>0.149437</td>\n      <td>0.165341</td>\n      <td>0.150000</td>\n      <td>0.011138</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1978.000000</td>\n      <td>0.274658</td>\n      <td>0.306717</td>\n      <td>0.273000</td>\n      <td>0.025489</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4766.000000</td>\n      <td>4.220601</td>\n      <td>3.274444</td>\n      <td>2.113000</td>\n      <td>0.745960</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('year == 2020')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in network.links:\n",
    "    network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==str(link.key)].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          counts    tt_avg\ncounts  1.000000  0.055122\ntt_avg  0.055122  1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n      <th>tt_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>counts</th>\n      <td>1.000000</td>\n      <td>0.055122</td>\n    </tr>\n    <tr>\n      <th>tt_avg</th>\n      <td>0.055122</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This correlation should be positive\n",
    "df[['counts','tt_avg']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            speed_sd  speed_avg       counts  total_obs\ndate                                                   \n2019-10-01  1.731787  17.175187  1770.335035      36195\n2019-10-02  1.760109  17.169768  1746.651824      36195\n2019-10-03  1.754288  17.092304  1785.115209      36195\n2019-10-08  1.847060  18.165569  1747.732955      36195\n2019-10-09  1.917923  18.137042  1756.834846      36195\n2019-10-10  1.830232  18.107925  1793.512340      36195\n2019-10-15  1.831527  18.114384  1750.339155      36195\n2019-10-16  1.823680  18.162625  1760.170975      36195\n2019-10-17  1.832219  18.080860  1775.411385      36195\n2019-10-22  1.837839  18.175561  1738.314834      36195\n2019-10-23  1.864443  18.222690  1753.243283      36195\n2019-10-24  1.848827  18.184814  1746.626881      36195\n2019-10-29  1.868430  18.167171  1731.246241      36195\n2019-10-30  1.816364  18.109232  1757.581055      36195\n2019-10-31  1.882446  18.128430  1804.595108      36195\n2020-10-01  1.290377  19.579215  1616.266052      36195\n2020-10-06  1.365919  19.709670  1581.200095      36195\n2020-10-07  1.327017  19.927088  1587.425957      36195\n2020-10-08  1.344382  19.969943  1604.360618      36195\n2020-10-13  1.311842  19.626350  1607.663262      36195\n2020-10-14  1.300075  19.558364  1613.399054      36195\n2020-10-15  1.331493  19.621633  1637.123641      36195\n2020-10-20  1.353743  19.597736  1599.044113      36195\n2020-10-21  1.325694  19.501768  1587.235083      36195\n2020-10-22  1.334320  19.591921  1628.075697      36195\n2020-10-27  1.309348  19.573460  1583.478061      36195\n2020-10-28  1.324270  19.646064  1593.821986      36195\n2020-10-29  1.336212  19.557279  1627.548269      36195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speed_sd</th>\n      <th>speed_avg</th>\n      <th>counts</th>\n      <th>total_obs</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-01</th>\n      <td>1.731787</td>\n      <td>17.175187</td>\n      <td>1770.335035</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-02</th>\n      <td>1.760109</td>\n      <td>17.169768</td>\n      <td>1746.651824</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-03</th>\n      <td>1.754288</td>\n      <td>17.092304</td>\n      <td>1785.115209</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-08</th>\n      <td>1.847060</td>\n      <td>18.165569</td>\n      <td>1747.732955</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-09</th>\n      <td>1.917923</td>\n      <td>18.137042</td>\n      <td>1756.834846</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-10</th>\n      <td>1.830232</td>\n      <td>18.107925</td>\n      <td>1793.512340</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>1.831527</td>\n      <td>18.114384</td>\n      <td>1750.339155</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>1.823680</td>\n      <td>18.162625</td>\n      <td>1760.170975</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>1.832219</td>\n      <td>18.080860</td>\n      <td>1775.411385</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-22</th>\n      <td>1.837839</td>\n      <td>18.175561</td>\n      <td>1738.314834</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-23</th>\n      <td>1.864443</td>\n      <td>18.222690</td>\n      <td>1753.243283</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-24</th>\n      <td>1.848827</td>\n      <td>18.184814</td>\n      <td>1746.626881</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-29</th>\n      <td>1.868430</td>\n      <td>18.167171</td>\n      <td>1731.246241</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-30</th>\n      <td>1.816364</td>\n      <td>18.109232</td>\n      <td>1757.581055</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-31</th>\n      <td>1.882446</td>\n      <td>18.128430</td>\n      <td>1804.595108</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-01</th>\n      <td>1.290377</td>\n      <td>19.579215</td>\n      <td>1616.266052</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-06</th>\n      <td>1.365919</td>\n      <td>19.709670</td>\n      <td>1581.200095</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-07</th>\n      <td>1.327017</td>\n      <td>19.927088</td>\n      <td>1587.425957</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-08</th>\n      <td>1.344382</td>\n      <td>19.969943</td>\n      <td>1604.360618</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-13</th>\n      <td>1.311842</td>\n      <td>19.626350</td>\n      <td>1607.663262</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-14</th>\n      <td>1.300075</td>\n      <td>19.558364</td>\n      <td>1613.399054</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-15</th>\n      <td>1.331493</td>\n      <td>19.621633</td>\n      <td>1637.123641</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-20</th>\n      <td>1.353743</td>\n      <td>19.597736</td>\n      <td>1599.044113</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-21</th>\n      <td>1.325694</td>\n      <td>19.501768</td>\n      <td>1587.235083</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-22</th>\n      <td>1.334320</td>\n      <td>19.591921</td>\n      <td>1628.075697</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-27</th>\n      <td>1.309348</td>\n      <td>19.573460</td>\n      <td>1583.478061</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-28</th>\n      <td>1.324270</td>\n      <td>19.646064</td>\n      <td>1593.821986</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-29</th>\n      <td>1.336212</td>\n      <td>19.557279</td>\n      <td>1627.548269</td>\n      <td>36195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              tt_sd    median_inc     incidents     bus_stops  intersections\ncount  1.013460e+06  1.013460e+06  1.013460e+06  1.013460e+06   1.013460e+06\nmean   1.813680e-02  2.621913e+01  7.441093e-01  1.500207e-01   8.765023e-01\nstd    2.783112e-02  2.135738e+01  3.193143e+00  4.411927e-01   1.319496e+00\nmin    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00\n25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00\n50%    1.113761e-02  2.482100e+01  0.000000e+00  0.000000e+00   0.000000e+00\n75%    2.548870e-02  4.168100e+01  0.000000e+00  0.000000e+00   1.000000e+00\nmax    7.459602e-01  1.158930e+02  4.000000e+01  4.000000e+00   9.000000e+00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tt_sd</th>\n      <th>median_inc</th>\n      <th>incidents</th>\n      <th>bus_stops</th>\n      <th>intersections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.813680e-02</td>\n      <td>2.621913e+01</td>\n      <td>7.441093e-01</td>\n      <td>1.500207e-01</td>\n      <td>8.765023e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.783112e-02</td>\n      <td>2.135738e+01</td>\n      <td>3.193143e+00</td>\n      <td>4.411927e-01</td>\n      <td>1.319496e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.113761e-02</td>\n      <td>2.482100e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.548870e-02</td>\n      <td>4.168100e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.459602e-01</td>\n      <td>1.158930e+02</td>\n      <td>4.000000e+01</td>\n      <td>4.000000e+00</td>\n      <td>9.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[_FEATURES_Z].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DTYPE = tf.float32\n",
    "\n",
    "# Data between 4pm and 5pm to estimate LUE, ODLUE and ODLULPE models\n",
    "X, Y = get_tensors_by_year(df[df.hour.isin([16])], features_Z = _FEATURES_Z, network = network)\n",
    "\n",
    "# Hourly data DURING morning and afternoon peak hour windows (6 hour intervals) to estimate TVODLULPE\n",
    "XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8, 15,16,17])], features_Z = _FEATURES_Z, network = network)\n",
    "\n",
    "# Split in training and test sets\n",
    "X_train, X_val, Y_train, Y_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [X[2019], X[2020], Y[2019], Y[2020]])\n",
    "XT_train, XT_val, YT_train, YT_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [XT[2019], XT[2020], YT[2019], YT[2020]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(90, 2413, 6), dtype=float64, numpy=\narray([[[ 0.0175, 93.828 ,  0.    ,  0.    ,  1.    ,  9.    ],\n        [ 0.0418, 66.653 ,  2.    ,  0.    ,  0.    ,  9.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    ,  9.    ],\n        ...,\n        [ 0.0009, 30.543 ,  1.    ,  0.    ,  1.    ,  9.    ],\n        [ 0.0007, 23.611 ,  6.    ,  0.    ,  2.    ,  9.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  9.    ]],\n\n       [[ 0.0268, 93.828 ,  0.    ,  0.    ,  1.    , 10.    ],\n        [ 0.035 , 66.653 ,  2.    ,  0.    ,  0.    , 10.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    , 10.    ],\n        ...,\n        [ 0.0009, 30.543 ,  1.    ,  0.    ,  1.    , 10.    ],\n        [ 0.0005, 23.611 ,  6.    ,  0.    ,  2.    , 10.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    , 10.    ]],\n\n       [[ 0.0305, 93.828 ,  0.    ,  0.    ,  1.    , 11.    ],\n        [ 0.0422, 66.653 ,  2.    ,  0.    ,  0.    , 11.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    , 11.    ],\n        ...,\n        [ 0.001 , 30.543 ,  1.    ,  0.    ,  1.    , 11.    ],\n        [ 0.0005, 23.611 ,  6.    ,  0.    ,  2.    , 11.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    , 11.    ]],\n\n       ...,\n\n       [[ 0.0179, 93.828 ,  0.    ,  0.    ,  1.    ,  0.    ],\n        [ 0.0244, 66.653 ,  2.    ,  0.    ,  0.    ,  0.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    ,  0.    ],\n        ...,\n        [ 0.0021, 30.543 ,  1.    ,  0.    ,  1.    ,  0.    ],\n        [ 0.003 , 23.611 ,  6.    ,  0.    ,  2.    ,  0.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ]],\n\n       [[ 0.0276, 93.828 ,  0.    ,  0.    ,  1.    ,  1.    ],\n        [ 0.0222, 66.653 ,  2.    ,  0.    ,  0.    ,  1.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    ,  1.    ],\n        ...,\n        [ 0.0018, 30.543 ,  1.    ,  0.    ,  1.    ,  1.    ],\n        [ 0.0025, 23.611 ,  6.    ,  0.    ,  2.    ,  1.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ]],\n\n       [[ 0.0199, 93.828 ,  0.    ,  0.    ,  1.    ,  2.    ],\n        [ 0.0315, 66.653 ,  2.    ,  0.    ,  0.    ,  2.    ],\n        [ 0.0255, 66.653 ,  6.    ,  0.    ,  0.    ,  2.    ],\n        ...,\n        [ 0.0008, 30.543 ,  1.    ,  0.    ,  1.    ,  2.    ],\n        [ 0.0026, 23.611 ,  6.    ,  0.    ,  2.    ,  2.    ],\n        [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  2.    ]]])>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XT[2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSS_WEIGHTS ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
    "_EQUILIBRIUM_STAGE = True\n",
    "_ALTERNATING_OPTIMIZATION = False\n",
    "\n",
    "_LR = {'learning': 1e-1, 'equilibrium': 1e-2}\n",
    "\n",
    "_BATCH_SIZE = 1 #16\n",
    "_EPOCHS = {'learning': 30, 'equilibrium': 30}\n",
    "# _EPOCHS = {'learning': 1, 'equilibrium': 1}\n",
    "# _XTICKS_SPACING = 2\n",
    "_XTICKS_SPACING = 5\n",
    "_EPOCHS_PRINT_INTERVAL = {'learning':1, 'equilibrium':1}\n",
    "\n",
    "_RELATIVE_GAP = 5e-2\n",
    "# _RELATIVE_GAP = 1e-1\n",
    "\n",
    "# _LOSS_METRIC  = mnrmse\n",
    "# _LOSS_METRIC  = nrmse\n",
    "# _LOSS_METRIC  = zscore\n",
    "_LOSS_METRIC  = z2score\n",
    "\n",
    "# With GPU\n",
    "_OPTIMIZERS = {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "              'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "val_results_dfs = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(network, homogenous = False, diagonal = False, adjacency_constraint = True, poly_order = 4,\n",
    "               alpha_prior = 0.15, beta_prior = 4, pretrain = False, dtype =_DTYPE, link_specific = True):\n",
    "\n",
    "    return MLP(n_links=len(network.links),\n",
    "               free_flow_traveltimes=[link.bpr.tf for link in network.links],\n",
    "               capacities=[link.bpr.k for link in network.links],\n",
    "               kernel_constraint=KernelConstraint(\n",
    "                   link_keys=[(link.key[0], link.key[1]) for link in network.links],\n",
    "                   dtype=dtype,\n",
    "                   capacities=[link.bpr.k for link in network.links],\n",
    "                   adjacency_constraint=adjacency_constraint,\n",
    "                   free_flow_traveltimes=[link.bpr.tf for link in network.links],\n",
    "                   diagonal= diagonal,\n",
    "                   homogenous=homogenous,\n",
    "                   bounds_clipping = [0,10],\n",
    "                   min_diagonal_value = 1e-1\n",
    "               ),\n",
    "               trainable =True,\n",
    "               polynomial_layer= PolynomialLayer(poly_order=poly_order,\n",
    "                                                 trainable = True,\n",
    "                                                 pretrain_weights=pretrain,\n",
    "                                                 alpha_prior = alpha_prior, beta_prior=beta_prior,\n",
    "                                                 kernel_constraint=tf.keras.constraints.NonNeg(),\n",
    "                                                 link_specific = link_specific\n",
    "                                                 ),\n",
    "               # alpha_relu = 1e-1,\n",
    "               alpha_relu = 0,\n",
    "               depth=1,\n",
    "               max_traveltime_factor = None,\n",
    "               dtype=dtype)\n",
    "\n",
    "def create_bpr(network, alpha_prior = 1, beta_prior = 1, dtype =_DTYPE):\n",
    "    return BPR(keys=['alpha', 'beta'],\n",
    "               # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "               # initial_values={'alpha': alpha_prior, 'beta': beta_prior}, # Consistent with MLP initialization\n",
    "               initial_values={'alpha': alpha_prior * tf.ones(len(network.links), dtype = dtype),\n",
    "                               'beta': beta_prior * tf.ones(len(network.links), dtype = dtype)},\n",
    "               true_values={'alpha': 0.15, 'beta': 4},\n",
    "               trainables={'alpha': True, 'beta':True},\n",
    "               capacities = [link.bpr.k for link in network.links],\n",
    "               free_flow_traveltimes =[link.bpr.tf for link in network.links],\n",
    "               dtype = dtype\n",
    "               )\n",
    "\n",
    "def create_model(network, model_key = 'tvgodlulpe', dtype=_DTYPE, n_periods=1, features_Z=_FEATURES_Z, historic_g=None,\n",
    "                                      performance_function=None, utility_parameters = None, od_parameters = None,\n",
    "                                      generation_parameters = None, generation = True, od_trainable = False,\n",
    "                                      utility_trainable = True, pretrain_generation_weights = True, generation_trainable = True):\n",
    "\n",
    "    if utility_parameters is None:\n",
    "        utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                               features_Z=features_Z,\n",
    "                                               # initial_values={\n",
    "                                               #                 'tt': -10,\n",
    "                                               #                 'tt_sd': -10, 'median_inc': 1,\n",
    "                                               #                 'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                               #                 'psc_factor': 0,\n",
    "                                               #                 'fixed_effect': np.zeros_like(network.links)},\n",
    "                                               initial_values={\n",
    "                                                   'tt': -3.0597,\n",
    "                                                   'tt_sd': -3.2678, 'median_inc': 0,\n",
    "                                                   'incidents': -4.5368, 'bus_stops': 0, 'intersections': -3.8788,\n",
    "                                                   'psc_factor': 0,\n",
    "                                                   'fixed_effect': np.zeros_like(network.links)},\n",
    "\n",
    "                                               signs={'tt': '-', 'tt_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                      'bus_stops': '-', 'intersections': '-'},\n",
    "                                               trainables={'psc_factor': False, 'fixed_effect': utility_trainable,\n",
    "                                                           'tt': utility_trainable, 'tt_sd': True, 'median_inc': True,\n",
    "                                                           'incidents': True,\n",
    "                                                           'bus_stops': True, 'intersections': True\n",
    "                                                           },\n",
    "                                               time_varying=True,\n",
    "                                               dtype=dtype\n",
    "                                               )\n",
    "\n",
    "    if performance_function is None:\n",
    "        # performance_function = create_bpr(network = network, dtype = dtype)\n",
    "        performance_function = create_mlp(network = network, dtype = dtype)\n",
    "\n",
    "    if generation_parameters is None and generation:\n",
    "        generation_parameters = GenerationParameters(\n",
    "            features_Z=['population', 'income', 'bus_stops'],\n",
    "            keys=['fixed_effect_od', 'fixed_effect_origin', 'fixed_effect_destination'],\n",
    "            initial_values={'income': 0, 'population': 0, 'bus_stops': 0,\n",
    "                            # 'fixed_effect': historic_g[0]\n",
    "                            'fixed_effect': historic_g\n",
    "                            },\n",
    "            signs={'income': '+', 'population': '+', 'bus_stops': '-'},\n",
    "            trainables={'fixed_effect': generation_trainable,\n",
    "                        'income': False, 'population': False, 'bus_stops': False,\n",
    "                        # 'income': True, 'population': True, 'bus_stops': True,\n",
    "                        'fixed_effect_origin': False, 'fixed_effect_destination': False, 'fixed_effect_od': generation_trainable\n",
    "                        # 'fixed_effect_origin': False, 'fixed_effect_destination': generation_trainable, 'fixed_effect_od': False\n",
    "                        },\n",
    "            # trainables={'fixed_effect': True, 'income': True, 'population': True, 'bus_stops': True},\n",
    "            # trainables={'fixed_effect': False, 'income': False, 'population': False, 'bus_stops': False},\n",
    "            time_varying=True,\n",
    "            # historic_g = od_parameters.compute_generated_trips(),\n",
    "            historic_g= historic_g,\n",
    "            pretrain_generation_weights=pretrain_generation_weights,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    if od_parameters is None:\n",
    "        od_parameters = ODParameters(key='od',\n",
    "                                     initial_values= q_historic,\n",
    "                                     historic_values={10: network.q.flatten()},\n",
    "                                     # total_trips={0: 1e5, 1: 1e5, 2: 1e5, 9: 1e5, 10: 1e5, 11: 1e5},\n",
    "                                     ods=network.ods,\n",
    "                                     n_periods=n_periods,\n",
    "                                     time_varying=True,\n",
    "                                     trainable= od_trainable,\n",
    "                                     )\n",
    "\n",
    "    model = NESUELOGIT(\n",
    "        key=model_key,\n",
    "        network=network,\n",
    "        dtype=dtype,\n",
    "        utility=utility_parameters,\n",
    "        performance_function=performance_function,\n",
    "        od=od_parameters,\n",
    "        generation=generation_parameters,\n",
    "        n_periods=n_periods\n",
    "    )\n",
    "\n",
    "    return model, {'utility_parameters': utility_parameters, 'generation_parameters': generation_parameters,\n",
    "                   'od_parameters': od_parameters, 'performance_function': performance_function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Total trips tvodlulpe pesuelogit:\n",
    "# Epoch 0: 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04\n",
    "# Final epoch: 6.4e+04 6.6e+04 6.3e+04 7.8e+04 7.9e+04 7.9e+04\n",
    "# Growth factor captures the difference between the reference OD at epoch 0 and the estimated OD.\n",
    "growth_factor = 7.9/6.6\n",
    "#growth_factor = 1\n",
    "\n",
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                              flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(q = q_historic, ods= network.ods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Benchmark (TVODLULPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "models['tvodlulpe'], _ = create_model(\n",
    "    model_key = 'tvodlulpe',\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE, alpha_prior = 0.9327, beta_prior = 4.1017),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "models['tvodlulpe'].key = 'tvodlulpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_dfs['tvodlulpe'], val_results_dfs['tvodlulpe'] = models['tvodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    node_data=nodes_df,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    loss_weights=_LOSS_WEIGHTS,\n",
    "    loss_metric= _LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvodlulpe'].save_weights(models['tvodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['tvodlulpe'].build()\n",
    "# models['tvodlulpe'].load_weights(\"output/models/230623133023_tvgodlulpe_fresno.h5\")\n",
    "# models['tvodlulpe'].update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flow_range = range(0,6000,100)\n",
    "sharey = False\n",
    "\n",
    "fig, axs = plot_performance_functions(model = models['tvodlulpe'],\n",
    "                                      network = network,\n",
    "                                      marginal = False,\n",
    "                                      #alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                                      #beta=models['tvodlulpe'].performance_function.beta,\n",
    "                                      alpha=0.15*np.ones(network.get_n_links()),\n",
    "                                      beta=4*np.ones(network.get_n_links()),\n",
    "                                      sharey = sharey,\n",
    "                                      flow_range = flow_range\n",
    "                                      # selected_links = np.random.choice(range(tntp_network.get_n_links()), 10, replace=False)\n",
    "                                      )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_xlim(xmin=-10)\n",
    "    ax.set_ylim(ymin=-1)\n",
    "    ax.legend(loc='upper left', title = 'period')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-comparison-all-link-performance-functions-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "selected_links = np.random.choice(range(network.get_n_links()), 10, replace=False)\n",
    "\n",
    "plot_performance_functions(model=models['tvodlulpe'],\n",
    "                           network=network,\n",
    "                           marginal=True,\n",
    "                           # alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           # beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           alpha=0.15*np.ones(network.get_n_links()),\n",
    "                           beta=4*np.ones(network.get_n_links()),\n",
    "                           selected_links = selected_links,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = models['tvodlulpe'],\n",
    "                        observed_traveltime=models['tvodlulpe'].mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= models['tvodlulpe'].mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            # curves=['travel time', 'link flow'],\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage = _EPOCHS['learning']\n",
    "                            )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-mse-tvodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['equilibrium'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'])\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-mape-tvodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=val_results_dfs['tvodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            curves=['equilibrium'],\n",
    "                            yaxis_label='mape (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_convergence_estimates(\n",
    "    estimates=train_results_dfs['tvodlulpe'].assign(\n",
    "        relative_gap=np.abs(train_results_dfs['tvodlulpe']['relative_gap']))[['epoch', 'relative_gap']],\n",
    "    xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "#ax.set_ylim(ymin=1e-2, ymax = 5.5e-2)\n",
    "#ax.set_ylim(ymin=1e-2)\n",
    "ax.set_xlim(xmin=-0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('relative gap (log scale)')\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-gap-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_convergence_estimates(estimates=train_results_dfs['tvodlulpe'][['epoch', 'alpha', 'beta']],\n",
    "                           xticks_spacing=_XTICKS_SPACING)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#sns.displot(plot_data, x=\"value\", hue=\"parameter\", multiple=\"stack\", kind=\"hist\", alpha=0.8, norm_hist=True)\n",
    "\n",
    "plot_data = pd.melt(pd.DataFrame({'alpha': models['tvodlulpe'].performance_function.alpha,\n",
    "                              'beta': models['tvodlulpe'].performance_function.beta}), var_name='parameter')\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "# sns.histplot(plot_data, x=\"value\", stat='density', hue=\"parameter\", alpha =0.8, ax = ax, binwidth = 0.5)\n",
    "\n",
    "sns.displot(plot_data, x=\"value\", kind=\"hist\", stat='percent', common_norm=False, hue = \"parameter\", alpha=0.8, facet_kws=dict(sharey=False, sharex = False), binwidth = 0.5)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-distribution-link-performance-parameters-tvodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "theta_df = plot_utility_parameters_periods(models['tvodlulpe'], period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "print(theta_df)\n",
    "#print(theta_df.values)\n",
    "\n",
    "plot_rr_by_period(models['tvodlulpe'],period_keys, model_key = 'tvodlulpe', period_feature='hour')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "sns.histplot(pd.DataFrame({'fixed_effect': np.array(models['tvodlulpe'].fixed_effect)}), stat='density', x=\"fixed_effect\", alpha=0.8, ax = ax, binwidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "theta_df = plot_utility_parameters_periods(models['tvodlulpe'], period_keys=period_keys, period_feature='hour', plot = False).reset_index().\\\n",
    "    drop(['hour','index'], axis = 1).reset_index().rename(columns = {'index':'period'})\n",
    "\n",
    "# theta_df = pd.melt(theta_df.assign(rr = theta_df.apply(compute_rr, axis=1)), id_vars = 'period', var_name = 'parameter').assign(group = 'utility').\\\n",
    "#     replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 'rr': 'reliability ratio', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income'})\n",
    "\n",
    "theta_df = pd.melt(theta_df, id_vars = 'period', var_name = 'parameter').assign(group = 'utility').\\\n",
    "    replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 'rr': 'reliability ratio', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income'})\n",
    "\n",
    "theta_df['hour'] = theta_df['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models['tvodlulpe'].period_dict.items()})\n",
    "\n",
    "fig, axs = plot_parameters(df = theta_df, n_cols_legend = 3, figsize = (6,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-utility-periods-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap with flows of top od pairs\n",
    "plot_top_od_flows_periods(models['tvodlulpe'],\n",
    "                          historic_od= network.q.flatten(),\n",
    "                          period_keys = period_keys,\n",
    "                          period_feature='hour', top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"theta = \"\n",
    "      f\"{dict(zip(models['tvodlulpe'].utility.true_values.keys(), list(np.mean(models['tvodlulpe'].theta.numpy(), axis=0))))}\")\n",
    "\n",
    "print(f\"alpha = {np.mean(models['tvodlulpe'].performance_function.alpha): 0.2f}, \"\n",
    "          f\"beta  = {np.mean(models['tvodlulpe'].performance_function.beta): 0.2f}\")\n",
    "\n",
    "print(f\"Avg abs diff of observed and estimated OD: \"\n",
    "      f\"{np.mean(np.abs(models['tvodlulpe'].q - network.q.flatten())): 0.2f}\")\n",
    "\n",
    "print(f\"Avg observed OD: {np.mean(np.abs(network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_df = models['tvodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                      X=XT_train, Y=YT_train).assign(dataset='training')\n",
    "metrics_df = pd.concat([metrics_df,\n",
    "                        models['tvodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                                 X=XT_val, Y=YT_val).assign(dataset='validation'),\n",
    "                        compute_benchmark_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score}, Y_ref=YT_train,\n",
    "                                                  Y=YT_val).assign(\n",
    "                            dataset='benchmark')\n",
    "                        ])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntvgodlulpe: Time specific utility and generation, and link specific parameters for performance functions')\n",
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "def create_tvgodlulpe_model():\n",
    "    return create_model(\n",
    "        n_periods = n_periods,\n",
    "        network = network,\n",
    "        performance_function = create_mlp(network = network,\n",
    "                                          poly_order = 4, pretrain = False, link_specific = False),\n",
    "        historic_g= generated_trips,\n",
    "        generation = True,\n",
    "        generation_trainable = True,\n",
    "        utility_trainable = True)\n",
    "\n",
    "models['tvgodlulpe'], _ = create_tvgodlulpe_model()\n",
    "models['tvgodlulpe'].key = 'tvgodlulpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# May remove validation set for faster training.\n",
    "train_results_dfs['tvgodlulpe'], val_results_dfs['tvgodlulpe'] = models['tvgodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # batch_size=None,\n",
    "    loss_weights= _LOSS_WEIGHTS,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvgodlulpe'].save_weights(models['tvgodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvgodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['tvgodlulpe'].build()\n",
    "# models['tvgodlulpe'].load_weights(\"output/models/230623142010_tvgodlulpe_fresno.h5\")\n",
    "# models['tvgodlulpe'].update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_links = np.random.choice(range(network.get_n_links()), 20, replace=False)\n",
    "\n",
    "#weights_interaction = models['tvgodlulpe'].performance_function.weights[1].numpy()\n",
    "weights_interaction = models['tvgodlulpe'].performance_function.model.layers[0].kernel_matrix.numpy()\n",
    "initial_weights_interaction = models['tvgodlulpe'].performance_function.kernel_constraint.initial_values.numpy()\n",
    "# Plot sign of values in flow interaction matrix\n",
    "plot_flow_interaction_matrix(\n",
    "    flow_interaction_matrix=weights_interaction,\n",
    "    masking_matrix = initial_weights_interaction,\n",
    "    # flow_interaction_matrix=np.where(weights_interaction>0, 1, -1)*np.where(weights_interaction==0, 0, 1),\n",
    "    # masking_matrix = np.where(initial_weights_interaction>0, 1, -1)*np.where(initial_weights_interaction==0, 0, 1),\n",
    "    links_ids = selected_links + 1,\n",
    "    # vmin = -0.05, vmax = 0.05\n",
    "    vmin = -1, vmax = 1\n",
    ")\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kernel-link-performance-functions-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Final weights interaction:\\n {weights_interaction}')\n",
    "print(f'\\nInitial weights interaction:\\n',initial_weights_interaction)\n",
    "\n",
    "# non_diagonal_mlp_weights = weights[~np.eye(weights.shape[0], dtype=bool)]\n",
    "# print(f\"Percentage of non-diagonal terms that are non-positive: {len(non_diagonal_mlp_weights[(non_diagonal_mlp_weights<=0)])/len(non_diagonal_mlp_weights):.2%}\")\n",
    "\n",
    "# non_zero_elements = weights_interaction[np.where(models['tvgodlulpe'].performance_function.kernel_constraint.initial_values.numpy()>0)]\n",
    "# print(f\"Percentage of non-zero terms that are positive: {len(non_zero_elements[(non_zero_elements>0)])/len(non_zero_elements):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of weight in interaction flow matrix. Elements that are set to zero by default are excluded from non-diagonal\n",
    "plot_data = pd.concat([pd.DataFrame({'value': tf.linalg.diag_part(weights_interaction).numpy().flatten(), 'parameter': 'diagonal'}),\n",
    "                                  pd.DataFrame({'value': weights_interaction[np.where(~np.eye(weights_interaction.shape[0],dtype=bool)*models['tvgodlulpe'].performance_function.kernel_constraint.adjacency_constraint_matrix)], 'parameter': 'non-diagonal'})])\n",
    "\n",
    "sns.displot(data = plot_data, x=\"value\", kind=\"hist\", stat='percent', common_norm=False, hue = \"parameter\", alpha=0.8,\n",
    "            facet_kws=dict(sharey=False, sharex = False), binwidth = 0.5) # , col=\"parameter\"\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-distribution-link-performance-parameters-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_data['value_abs'] = plot_data.value.abs()\n",
    "\n",
    "print(plot_data.groupby('parameter')[['value', 'value_abs']].mean())\n",
    "print(plot_data.groupby('parameter')[['value']].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of polynomial\n",
    "print(f\"polynomial weights {tf.reduce_mean(models['tvgodlulpe'].performance_function.polynomial_layer.poly_weights,0).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "flow_range = range(0,6000,100)\n",
    "sharey = False\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = False,\n",
    "                           # alpha=models['tvgodlulpe'].performance_function.polynomial_layer.alpha_prior,\n",
    "                           # beta=models['tvgodlulpe'].performance_function.polynomial_layer.beta_prior,\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-comparison-all-link-performance-functions-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "selected_links = np.random.choice(range(network.get_n_links()), 10, replace=False)\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = False,\n",
    "                           selected_links = selected_links,\n",
    "                           # alpha=models['tvgodlulpe'].performance_function.polynomial_layer.alpha_prior,\n",
    "                           # beta=models['tvgodlulpe'].performance_function.polynomial_layer.beta_prior,\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot_performance_functions(model = models['tvgodlulpe'],\n",
    "#                                network = network,\n",
    "#                                marginal = False,\n",
    "#                                selected_links = selected_links,\n",
    "#                                alpha = 0.15,\n",
    "#                                beta = 4,\n",
    "#                                palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "#                                sharey = sharey,\n",
    "#                                flow_range = flow_range\n",
    "#                                )\n",
    "# plt.show()\n",
    "\n",
    "plot_performance_functions(model = models['tvgodlulpe'],\n",
    "                           network = network,\n",
    "                           marginal = True,\n",
    "                           selected_links = selected_links,\n",
    "                           # alpha = 0.15*np.ones(network.get_n_links()),\n",
    "                           # beta = 4*np.ones(network.get_n_links()),\n",
    "                           alpha=models['tvodlulpe'].performance_function.alpha,\n",
    "                           beta=models['tvodlulpe'].performance_function.beta,\n",
    "                           palette = sns.color_palette(\"hls\", len(selected_links)),\n",
    "                           sharey = sharey,\n",
    "                           flow_range = flow_range\n",
    "                           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = models['tvgodlulpe'],\n",
    "                        observed_traveltime=models['tvgodlulpe'].mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= models['tvgodlulpe'].mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            # curves=['travel time', 'link flow'],\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage = _EPOCHS['learning']\n",
    "                            )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-mse-tvgodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['equilibrium'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'])\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            curves=['travel time', 'link flow'],\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-mape-tvgodlulpe.png')\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs['tvgodlulpe'], val_losses=val_results_dfs['tvgodlulpe'],\n",
    "                            xticks_spacing=_XTICKS_SPACING, show_validation=False,\n",
    "                            epochs_end_learning_stage=_EPOCHS['learning'], prefix_metric='mape',\n",
    "                            curves=['equilibrium'],\n",
    "                            yaxis_label='mape (%)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_convergence_estimates(\n",
    "    estimates=train_results_dfs['tvgodlulpe'].assign(\n",
    "        relative_gap=np.abs(train_results_dfs['tvgodlulpe']['relative_gap']))[['epoch', 'relative_gap']],\n",
    "    xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "#ax.set_ylim(ymin=1e-2)\n",
    "# ax.set_ylim(ymin=1e-2, ymax = 5.5e-2)\n",
    "ax.set_xlim(xmin=-0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('relative gap (log scale)')\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-relative-gap-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "theta_df = plot_utility_parameters_periods(models['tvgodlulpe'], period_keys = period_keys, period_feature='hour')\n",
    "#print(theta_df.values)\n",
    "\n",
    "plot_rr_by_period(models['tvgodlulpe'], period_keys, model_key = 'tvgodlulpe', period_feature='hour')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (5,4))\n",
    "sns.histplot(pd.DataFrame({'fixed_effect': np.array(models['tvodlulpe'].fixed_effect)}), stat='density', x=\"fixed_effect\", alpha=0.8, ax = ax, binwidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot parameters by period\n",
    "theta_df = plot_utility_parameters_periods(models['tvgodlulpe'], period_keys=period_keys, period_feature='hour', plot = False).reset_index().\\\n",
    "    drop(['hour','index'], axis = 1).reset_index().rename(columns = {'index':'period'})\n",
    "\n",
    "# theta_df = pd.melt(theta_df.assign(rr = theta_df.apply(compute_rr, axis=1)), id_vars = 'period', var_name = 'parameter').assign(group = 'utility')\n",
    "\n",
    "theta_df = pd.melt(theta_df, id_vars = 'period', var_name = 'parameter').assign(group = 'utility')\n",
    "\n",
    "kappa_df = pd.concat([pd.DataFrame({'period': [period], 'parameter': [feature], 'value': [models['tvgodlulpe'].kappa[period, i].numpy()]}) for period in range(models['tvgodlulpe'].kappa.shape[0]) for i, feature in enumerate(models['tvgodlulpe'].generation.features)]).assign(group = 'generation')\n",
    "\n",
    "parameters_df = pd.concat([theta_df, kappa_df]).\\\n",
    "    replace({'tt': 'travel time', 'tt_sd': 'std. travel time', 's': 'intersections', 'bus_stops': 'bus stops', 'median_inc': 'income', 'rr': 'reliability ratio'})\n",
    "\n",
    "parameters_df['hour'] = parameters_df['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models['tvgodlulpe'].period_dict.items()})\n",
    "\n",
    "fig, ax = plot_parameters(df = parameters_df[parameters_df.group == 'utility'], n_cols_legend = 3, figsize = (6,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-utility-periods-tvgodlulpe.png')\n",
    "\n",
    "plot_parameters(df = parameters_df[parameters_df.group == 'generation'], n_cols_legend = 3, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-generation-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Theta:', models['tvgodlulpe'].theta.numpy().T)\n",
    "print('Kappa:', models['tvgodlulpe'].kappa.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap with flows of top od pairs\n",
    "plot_top_od_flows_periods(models['tvgodlulpe'],\n",
    "                          historic_od= network.q.flatten(),\n",
    "                          period_keys = period_keys,\n",
    "                          period_feature='hour', top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"theta = \"\n",
    "      f\"{dict(zip(models['tvgodlulpe'].utility.true_values.keys(), list(np.mean(models['tvgodlulpe'].theta.numpy(), axis=0))))}\")\n",
    "print(f\"kappa= \"\n",
    "      f\"{dict(zip(models['tvgodlulpe'].generation.features, list(np.mean(models['tvgodlulpe'].kappa.numpy(), axis=0))))}\")\n",
    "\n",
    "if models['tvgodlulpe'].performance_function.type == 'bpr':\n",
    "    print(f\"alpha = {np.mean(models['tvgodlulpe'].performance_function.alpha): 0.2f}, \"\n",
    "          f\"beta  = {np.mean(models['tvgodlulpe'].performance_function.beta): 0.2f}\")\n",
    "\n",
    "print(f\"Avg abs diff of observed and estimated OD: \"\n",
    "      f\"{np.mean(np.abs(models['tvgodlulpe'].q - network.q.flatten())): 0.2f}\")\n",
    "\n",
    "print(f\"Avg observed OD: {np.mean(np.abs(network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = models['tvgodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                      X=XT_train, Y=YT_train).assign(dataset='training')\n",
    "# metrics_df = pd.concat([metrics_df,\n",
    "#                         models['tvgodlulpe'].compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "#                                                                  X=XT_val, Y=YT_val).assign(dataset='validation'),\n",
    "#                         compute_benchmark_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score}, Y_ref=YT_train,\n",
    "#                                                   Y=YT_val).assign(\n",
    "#                             dataset='benchmark')\n",
    "#                         ])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Optimal equilibrium hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_RELATIVE_GAP_HP = 0\n",
    "_EPOCHS_HP = {'learning': 30, 'equilibrium': 0}\n",
    "_N_SPLITS_HP = 3\n",
    "\n",
    "target_metric = 'mse'\n",
    "target_component = 'flow'\n",
    "\n",
    "# grid_equilibrium = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "grid_equilibrium = [1e-1, 5e-1, 1, 2]\n",
    "loss_weights = []\n",
    "for i in grid_equilibrium:\n",
    "    loss_weights.append(_LOSS_WEIGHTS.copy())\n",
    "    loss_weights[-1]['equilibrium'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = compute_generated_trips(q=tf.stack(q_historic), ods=network.ods)\n",
    "\n",
    "model, _ = create_model(\n",
    "    n_periods = n_periods,\n",
    "    network = network,\n",
    "    performance_function = create_mlp(network = network,\n",
    "                                      poly_order = 4, pretrain = False, link_specific = False),\n",
    "    historic_g= generated_trips,\n",
    "    generation = True,\n",
    "    generation_trainable = True,\n",
    "    utility_trainable = True)\n",
    "model.key = 'tvgodlulpe'\n",
    "\n",
    "regularization_metrics_df, optimal_weights, optimal_metrics_kfold_df, optimal_parameters_kfold_df \\\n",
    "    = regularization_kfold(\n",
    "    loss_weights=loss_weights,\n",
    "    target_metric = 'mse',\n",
    "    target_component = 'flow',\n",
    "    n_splits=_N_SPLITS_HP,\n",
    "    random_state=_SEED,\n",
    "    model=model,\n",
    "    X=XT_train, Y=YT_train,\n",
    "    optimizers=_OPTIMIZERS,\n",
    "    # generalization_error={'train': False, 'validation': True},\n",
    "    node_data=nodes_df,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows=True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP_HP,\n",
    "    # batch_size=1,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5},\n",
    "    epochs=_EPOCHS_HP,\n",
    "    # epochs_print_interval = _EPOCHS,\n",
    "    # epochs_print_interval = {'learning': 30, 'equilibrium': 30},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "filepath = f\"output/tables/{ts}_regularization_sensitivity_{'fresno'}.csv\"\n",
    "regularization_metrics_df.to_csv(filepath, index=False)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "regularization_plot_df = pd.read_csv(filepath)\n",
    "regularization_plot_df = regularization_plot_df.sort_values(by = ['component', 'lambda_equilibrium', 'dataset'])\n",
    "regularization_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in validation set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'validation') & (regularization_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'validation') & (regularization_plot_df.component == 'flow') ]['value'])\n",
    "z = regularization_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = regularization_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-20, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in training set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'training') & (regularization_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(regularization_plot_df[(regularization_plot_df.dataset == 'training') & (regularization_plot_df.component == 'flow') ]['value'])\n",
    "z = regularization_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = regularization_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-25, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "print(generation_factors)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "growth_factor = 7.9/6.6\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(q = network.q.flatten()[np.newaxis,:], ods= network.ods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TVODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['reference-model'], _ = create_model(\n",
    "#     n_periods=n_periods, network=network, historic_g=generation_factors.values[:, np.newaxis] * generated_trips)\n",
    "#\n",
    "# train_results_dfs['reference-model'], val_results_dfs['reference-model'] = models['reference-model'].fit(\n",
    "#     XT_train, YT_train, XT_val, YT_val,\n",
    "#     node_data=nodes_df,\n",
    "#     optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "#                  'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "#                  },\n",
    "#     batch_size=_BATCH_SIZE,\n",
    "#     # loss_weights= _LOSS_WEIGHTS,\n",
    "#     loss_weights = {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1},\n",
    "#     loss_metric=_LOSS_METRIC,\n",
    "#     equilibrium_stage= True,\n",
    "#     alternating_optimization= False,\n",
    "#     pretrain_link_flows = True,\n",
    "#     threshold_relative_gap= 2e-1,\n",
    "#     epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "#     epochs=_EPOCHS)\n",
    "\n",
    "reference_model = models['tvodlulpe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model, _ = create_model(\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE, alpha_prior = 0.9327, beta_prior = 4.1017),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inference_model.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "inference_model.load_weights(reference_model._filepath_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Model estimated with all data from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= 1,\n",
    "                        # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                        # batch_size= None,\n",
    "                        optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        # threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                        threshold_relative_gap=_RELATIVE_GAP,\n",
    "                        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                        epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TVGODLULPE is retrained with optimal hyperparameter weight for equilibrium\n",
    "\n",
    "models['tvgodlulpe-optimal'], _ = create_tvgodlulpe_model()\n",
    "#optimal_weights ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_dfs['tvgodlulpe-optimal'], val_results_dfs['tvgodlulpe-optimal'] = models['tvgodlulpe-optimal'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])},\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    # batch_size=None,\n",
    "    loss_weights= optimal_weights,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP_HP,\n",
    "    epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "    epochs=_EPOCHS_HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models['reference-model'], _ = create_model(\n",
    "#     n_periods=n_periods, network=network, historic_g=generation_factors.values[:, np.newaxis] * generated_trips)\n",
    "#\n",
    "# train_results_dfs['reference-model'], val_results_dfs['reference-model'] = models['reference-model'].fit(\n",
    "#     XT_train, YT_train, XT_val, YT_val,\n",
    "#     node_data=nodes_df,\n",
    "#     optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "#                  'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "#                  },\n",
    "#     batch_size=_BATCH_SIZE,\n",
    "#     # loss_weights= _LOSS_WEIGHTS,\n",
    "#     loss_weights = {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1},\n",
    "#     loss_metric=_LOSS_METRIC,\n",
    "#     equilibrium_stage= True,\n",
    "#     alternating_optimization= False,\n",
    "#     pretrain_link_flows = True,\n",
    "#     threshold_relative_gap= 2e-1,\n",
    "#     epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "#     epochs=_EPOCHS)\n",
    "\n",
    "reference_model = models['tvgodlulpe-optimal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model, _ = create_tvgodlulpe_model()\n",
    "# inference_model, _ = create_model(\n",
    "#     n_periods = n_periods,\n",
    "#     network = network,\n",
    "#     performance_function = create_mlp(network = network,\n",
    "#                                       poly_order = 4, alpha_prior = 1, beta_prior = 4, pretrain = False, link_specific = False),\n",
    "#     historic_g= generated_trips,\n",
    "#     generation = True,\n",
    "#     utility_trainable = True)\n",
    "\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inference_model.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "inference_model.load_weights(reference_model._filepath_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Model estimated with all data from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= 1,\n",
    "                        # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                        # batch_size= None,\n",
    "                        optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        # threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                        threshold_relative_gap=_RELATIVE_GAP,\n",
    "                        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                        epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={'mape': mape, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= 1,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            # batch_size= None,\n",
    "                            # optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=1e-1),\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=5e-2,  # _RELATIVE_GAP,\n",
    "                            epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {'mape': mape, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to mask observed travel time and observed flow\n",
    "\n",
    "predictions = pd.DataFrame({'link_key': list(network.links_keys) * Y_train.shape[0],\n",
    "                            'observed_traveltime': Y_train[:, :, 0].numpy().flatten(),\n",
    "                            'observed_flow': Y_train[:, :, 1].numpy().flatten()})\n",
    "\n",
    "predictions['date'] = sorted(df[df.hour == 16].loc[df[df.hour == 16].year == 2019, 'date'])\n",
    "\n",
    "# # TODO: Write predictions for TVODLULPE model\n",
    "# for model in [lue,odlue,odlulpe]:\n",
    "#\n",
    "#     predicted_flows = model.flows()\n",
    "#     predicted_traveltimes = model.traveltimes()\n",
    "#\n",
    "#     predictions['predicted_traveltime_' + model.key] = np.tile(predicted_traveltimes, (Y_train.shape[0], 1)).flatten()\n",
    "#     predictions['predicted_flow_' + model.key] = np.tile(predicted_flows, (Y_train.shape[0], 1)).flatten()\n",
    "#\n",
    "# predictions.to_csv(f\"./output/tables/{datetime.now().strftime('%y%m%d%H%M%S')}_train_predictions_{network.key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold = {}\n",
    "metrics_kfold_df = {}\n",
    "parameters_kfold_df = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - TVODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold['tvodlulpe'], _ = create_model(\n",
    "    n_periods= n_periods, network = network,\n",
    "    performance_function = create_bpr(network = network, dtype = _DTYPE),\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 #initial_values= generation_factors.values[:,np.newaxis]*tntp_network.q.flatten(),\n",
    "                                 initial_values = tf.stack(q_historic),\n",
    "                                 # historic_values={10: q_historic[0].flatten()},\n",
    "                                 ods=network.ods,\n",
    "                                 n_periods=n_periods,\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True),\n",
    "    generation = False,\n",
    "    utility_trainable = True\n",
    ")\n",
    "\n",
    "models_kfold['tvodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "#models_kfold['tvodlulpe'].generation._pretrain_generation_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Use pretrained weights\n",
    "models_kfold['tvodlulpe'].load_weights(models['tvodlulpe']._filepath_weights)\n",
    "# model_kfold.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "# model_kfold.load_weights(\"output/models/230616191846_tvgodlulpe_fresno.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "# q_reference = models['tvgodlulpe'].q\n",
    "q_reference = models_kfold['tvodlulpe'].q\n",
    "\n",
    "q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvodlulpe'].od.historic_values = q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization.\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'], parameters_kfold_df['tvodlulpe'] = train_kfold(\n",
    "    n_splits=10,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows = False,\n",
    "    # threshold_relative_gap= 5e-2,\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=1,\n",
    "    epochs = _EPOCHS,\n",
    "    # epochs={'learning': 30, 'equilibrium': 10},\n",
    "    # batch_size=None,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5}\n",
    "    # epochs_print_interval= {'learning': 100, 'equilibrium': 100},\n",
    "    # epochs= {'learning': 4, 'equilibrium': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'].to_csv(f\"./output/experiments/{datetime.now().strftime('%y%m%d%H%M%S')}_kfold_{network.key}.csv\")\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])], metric_name = 'mape', showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-mape-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvodlulpe']['hour'] = parameters_kfold_df['tvodlulpe'].period.replace({v: k for k, v in models_kfold['tvodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['parameter'] = parameters_kfold_df['tvodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe'] = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### - TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models_kfold['tvgodlulpe'], _ = create_model(\n",
    "    n_periods = n_periods,\n",
    "    network = network,\n",
    "    performance_function = create_mlp(network = network,\n",
    "                                      poly_order = 4, pretrain = False, link_specific = False),\n",
    "    historic_g= generated_trips,\n",
    "    generation = True,\n",
    "    generation_trainable = True,\n",
    "    utility_trainable = True)\n",
    "\n",
    "models_kfold['tvgodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "models_kfold['tvgodlulpe'].generation._pretrain_generation_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Use pretrained weights\n",
    "models_kfold['tvgodlulpe'].load_weights(models['tvgodlulpe']._filepath_weights)\n",
    "# model_kfold.load_weights(\"output/models/230616010949_tvgodlulpe_fresno.h5\")\n",
    "# model_kfold.load_weights(\"output/models/230616191846_tvgodlulpe_fresno.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "# q_reference = models['tvgodlulpe'].q\n",
    "q_reference = models_kfold['tvgodlulpe'].q\n",
    "\n",
    "q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvgodlulpe'].od.historic_values = q_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization.\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvgodlulpe'], parameters_kfold_df['tvgodlulpe'] = train_kfold(\n",
    "    n_splits=10,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvgodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows = False,\n",
    "    # threshold_relative_gap= 5e-2,\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=1,\n",
    "    epochs = _EPOCHS,\n",
    "    # epochs={'learning': 30, 'equilibrium': 10},\n",
    "    # batch_size=None,\n",
    "    # epochs={'learning': 3, 'equilibrium': 5}\n",
    "    # epochs_print_interval= {'learning': 100, 'equilibrium': 100},\n",
    "    # epochs= {'learning': 4, 'equilibrium': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvgodlulpe'].to_csv(f\"./output/experiments/{datetime.now().strftime('%y%m%d%H%M%S')}_kfold_{network.key}.csv\")\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])], metric_name = 'mape', showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-mape-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvgodlulpe']['hour'] = parameters_kfold_df['tvgodlulpe'].period.replace({v: k for k, v in models_kfold['tvgodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['parameter'] = parameters_kfold_df['tvgodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvgodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvgodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe'] = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'generation'], n_cols_legend = 3, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-generation-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write estimation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_df, val_results_df \\\n",
    "    = map(lambda x: pd.concat([results.assign(model = model)[['model'] + list(results.columns)]\n",
    "                               for model, results in x.items()],axis = 0), [train_results_dfs, val_results_dfs])\n",
    "\n",
    "train_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_train_results_{network.key}.csv\"\n",
    "val_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_validation_results_{network.key}.csv\"\n",
    "train_results_df.to_csv(f\"./output/tables/{train_filename}\")\n",
    "print(f'File {train_filename} was written')\n",
    "val_results_df.to_csv(f\"./output/tables/{val_filename}\")\n",
    "print(f'File {val_filename} was written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of parameter estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'parameter': [], 'model': []})\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    model_results = {**dict(zip(['tt'] + _FEATURES_Z, list(np.mean(model.theta.numpy(), axis=0)))),\n",
    "                     **{'rr': float(model.get_parameters_estimates().eval('tt_sd/tt').iloc[0]),\n",
    "                        'fixed_effect_mean': np.mean(model.fixed_effect),\n",
    "                        'fixed_effect_std': np.std(model.fixed_effect),\n",
    "                        'od_mean': np.mean(model.q),\n",
    "                        'od_std': np.std(model.q)\n",
    "                        }}\n",
    "\n",
    "    if model.performance_function.type == 'bpr':\n",
    "        model_results = {**model_results, **{'alpha_mean': np.mean(model.performance_function.alpha),\n",
    "                                             'alpha_std': np.std(model.performance_function.alpha),\n",
    "                                             'beta_mean': np.mean(model.performance_function.beta),\n",
    "                                             'beta_std': np.std(model.performance_function.beta)}}\n",
    "\n",
    "    model_results = pd.DataFrame({'parameter': model_results.keys(), 'values': model_results.values()}). \\\n",
    "        assign(model=model_key)\n",
    "\n",
    "    results = pd.concat([results, model_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.pivot_table(index = ['parameter'], columns = 'model', values = 'values', sort=False).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### TVGODLULPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Stats by period\n",
    "model = models['tvgodlulpe']\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('theta:', pd.DataFrame(model.theta.numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('kappa:', pd.DataFrame(model.kappa.numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print('OD matrix', pd.DataFrame({'od_mean': np.mean(model.q,axis = 1), 'od_std': np.std(model.q, axis = 1), 'od_total':np.sum(model.q,axis = 1)}).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of models goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_losses = pd.DataFrame({})\n",
    "loss_columns = ['loss_flow', 'loss_traveltime', 'loss_equilibrium', 'loss_total']\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    results_losses_model = model.split_results(train_results_dfs[model_key])[1].assign(model=model_key)\n",
    "    results_losses_model = results_losses_model[results_losses_model.epoch == _EPOCHS['learning']].iloc[[0]]\n",
    "    results_losses = pd.concat([results_losses, results_losses_model])\n",
    "\n",
    "results_losses[loss_columns] = (results_losses[loss_columns] - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print(results_losses[['model'] + list(results_losses.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of convergence toward true rr across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimates = {}\n",
    "train_losses = {}\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    train_estimates[model_key], train_losses[model_key] = model.split_results(results=train_results_dfs[model_key])\n",
    "\n",
    "    train_estimates[model_key]['model'] = model_key\n",
    "\n",
    "train_estimates_df = pd.concat(train_estimates.values())\n",
    "\n",
    "train_estimates_df['rr'] = train_estimates_df['tt_sd'] / train_estimates_df['tt']\n",
    "\n",
    "estimates = train_estimates_df[['epoch', 'model', 'rr']].reset_index().drop('index', axis=1)\n",
    "#estimates = estimates[estimates.epoch != 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "g = sns.lineplot(data=estimates, x='epoch', hue='model', y='rr')\n",
    "\n",
    "# ax.hlines(y=compute_rr(models.popitem().true_values), xmin=estimates['epoch'].min(), xmax=estimates['epoch'].max(), linestyle='--', label = 'truth')\n",
    "\n",
    "ax.set_ylabel('reliability ratio')\n",
    "\n",
    "ax.set_xticks(np.arange(estimates['epoch'].min(), estimates['epoch'].max() + 2, _XTICKS_SPACING))\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "#plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of reliability ratio by hour for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_ratios = plot_rr_by_period_models(models, period_keys, period_feature='hour', join = True)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-reliability-ratios-periods.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(reliability_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "    print(reliability_ratios[reliability_ratios.model == 'tvgodlulpe'][['hour','rr']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reliability_ratios.groupby('model')[['rr']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of total trips by hour for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips = plot_total_trips_models(models = models, period_feature = 'hour', period_keys = period_keys,\n",
    "                                      historic_od = network.q.flatten(), join = True)\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-total-trips-periods.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(total_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips.groupby('model')[['total_trips']].mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips.groupby('model')[['total_trips']].sum().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Global runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'runtime: {time.time()-t0_global:0.1f} [s]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nesuelogit",
   "language": "python",
   "name": "nesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
