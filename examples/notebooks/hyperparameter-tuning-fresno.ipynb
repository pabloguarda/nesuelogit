{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.colors as colors\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import random\n",
    "import isuelogit as isl\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath(\"\")).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "sys.path.append(os.path.join(main_dir, 'src'))\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pesuelogit.networks import read_OD, load_k_shortest_paths\n",
    "from pesuelogit.etl import data_curation, add_period_id\n",
    "\n",
    "# Functions from internal modules\n",
    "from nesuelogit.models import compute_generated_trips, compute_generation_factors, \\\n",
    "    regularization_kfold, create_tvgodlulpe_model_fresno\n",
    "from nesuelogit.etl import build_network, get_tensors_by_year\n",
    "from nesuelogit.visualizations import plot_flow_vs_traveltime, plot_congestion_maps\n",
    "from nesuelogit.metrics import mse, mape, r2_score,  z2score, mdape\n",
    "from nesuelogit.utils import read_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report global runtime\n",
    "t0_global = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set timestamp to add in the filenames that are written in disk\n",
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "print('Timestamp:',ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "isl.config.dirs['read_network_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read nodes and link-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv('./input/network-data/fresno/nodes/fresno-nodes-gis-data.csv')\n",
    "\n",
    "links_df = pd.read_csv('./input/network-data/fresno/links/fresno-link-specific-data.csv',\n",
    "                       converters={\"link_key\": ast.literal_eval, \"pems_id\": ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Display network\n",
    "links_gdf = gpd.read_file('./input/network-data/fresno/gis/links/fresno-links-gis.shp').set_crs(\n",
    "        'EPSG:2228')\n",
    "ax = links_gdf.to_crs(epsg=3857).plot(figsize=(10, 10), alpha=0.5)\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "network = build_network(links_df=links_df, nodes_df=nodes_df, crs='epsg:4326', key= 'fresno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_OD(network=network, sparse=True)\n",
    "\n",
    "q_historic = np.repeat(network.q.flatten()[np.newaxis, :], 6, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_paths(network=network, update_incidence_matrices=True, filename = 'paths-fresno-k3.csv')\n",
    "# read_paths(network=network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = './input/network-data/fresno/links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "df['link_key'] = pd.Categorical(df['link_key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "df['period'] = pd.to_datetime(df['period'], format = '%Y-%m-%d-%H').dt.strftime('%Y-%m-%d-%H')\n",
    "\n",
    "# Select data from Tuesdays to Thursdays\n",
    "df = df[df['date'].dt.dayofweek.between(1, 3)]\n",
    "\n",
    "# Select data from first Tuesdays of 2019 and 2020\n",
    "# df = df[df['date'].isin([\"2019-10-01\", \"2020-10-06\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add period id for timevarying estimation\n",
    "period_feature = 'hour'\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)\n",
    "\n",
    "df['tt_sd'] = df['tt_sd_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = nodes_df.rename(columns ={'pop_tract':'population','stops_tract': 'bus_stops','median_inc':'income'})\n",
    "\n",
    "features_generation = ['population','income', 'bus_stops']\n",
    "\n",
    "nodes_df = nodes_df[['key','type'] + features_generation]\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(nodes_df[features_generation])\n",
    "nodes_df[features_generation] = imp_mean.transform(nodes_df[features_generation])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(nodes_df[features_generation].values)\n",
    "nodes_df[features_generation] = scaler.transform(nodes_df[features_generation].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURES_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in network.links:\n",
    "    network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==link.key].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[_FEATURES_Z].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DTYPE = tf.float32\n",
    "\n",
    "X, Y = {}, {}\n",
    "\n",
    "# Data between 4pm and 5pm to estimate LUE, ODLUE and ODLULPE models\n",
    "X, Y = get_tensors_by_year(df[df.hour.isin([16])], features_Z = _FEATURES_Z, links_keys=list(network.links_dict.keys()))\n",
    "\n",
    "# Hourly data DURING morning and afternoon peak hour windows (6 hour intervals) to estimate TVODLULPE\n",
    "XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8, 15,16,17])], features_Z = _FEATURES_Z, links_keys=list(network.links_dict.keys()))\n",
    "\n",
    "# Split in training and test sets\n",
    "X_train, X_val, Y_train, Y_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [X[2019], X[2020], Y[2019], Y[2020]])\n",
    "XT_train, XT_val, YT_train, YT_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [XT[2019], XT[2020], YT[2019], YT[2020]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Critical hyperparameters\n",
    "_EPOCHS = {'learning':30}\n",
    "_BATCH_SIZE = 1\n",
    "\n",
    "# Number of splits for k-fold method\n",
    "_N_SPLITS_HP = 5\n",
    "_GRID_EQUILIBRIUM_HP = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "# _GRID_EQUILIBRIUM_HP = [1e-1, 5e-1, 1, 2]\n",
    "# _GRID_EQUILIBRIUM_HP = [1e-1, 5e-1]\n",
    "\n",
    "# These hyperparameters can be left in their current values\n",
    "_LOSS_WEIGHTS ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
    "_EQUILIBRIUM_STAGE = False\n",
    "_ALTERNATING_OPTIMIZATION = False\n",
    "_RELATIVE_GAP = 0\n",
    "_LR = {'learning': 1e-1}\n",
    "_LOSS_METRIC  = z2score\n",
    "_EVALUATION_METRIC = mdape\n",
    "_OPTIMIZERS = {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "val_results_dfs = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Growth factor captures the difference between the reference OD at epoch 0 and the estimated OD.\n",
    "growth_factor = 7.9/6.6 # 1\n",
    "\n",
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                              flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*\\\n",
    "                  compute_generated_trips(q = q_historic, ods= network.ods, n_nodes = len(network.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Search of optimal hyperparameter weighting the equilibrium component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "target_metric = 'mse'\n",
    "target_component = 'flow'\n",
    "\n",
    "loss_weights = []\n",
    "\n",
    "if isinstance(_GRID_EQUILIBRIUM_HP, (int, float)):\n",
    "    _GRID_EQUILIBRIUM_HP = [_GRID_EQUILIBRIUM_HP]\n",
    "\n",
    "for i in _GRID_EQUILIBRIUM_HP:\n",
    "    loss_weights.append(_LOSS_WEIGHTS.copy())\n",
    "    loss_weights[-1]['equilibrium'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "generated_trips = compute_generated_trips(q=tf.stack(q_historic), ods=network.ods, n_nodes = len(network.nodes))\n",
    "\n",
    "model, _ = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods,\n",
    "                                                      historic_q = q_historic, features_Z = _FEATURES_Z)\n",
    "\n",
    "hp_metrics_df, optimal_weights, optimal_metrics_kfold_df, optimal_parameters_kfold_df \\\n",
    "    = regularization_kfold(\n",
    "    loss_weights=loss_weights,\n",
    "    target_metric = 'mse',\n",
    "    target_component = 'flow',\n",
    "    n_splits=_N_SPLITS_HP,\n",
    "    random_state=_SEED,\n",
    "    model=model,\n",
    "    X=XT_train, Y=YT_train,\n",
    "    optimizers=_OPTIMIZERS,\n",
    "    node_data=nodes_df,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    epochs_print_interval = _EPOCHS,\n",
    "    #equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    pretrain_link_flows=True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    epochs=_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filepath = f\"output/tables/{ts}_hyperparameter_tuning_{'fresno'}.csv\"\n",
    "hp_metrics_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hp_plot_df = pd.read_csv(filepath)\n",
    "hp_plot_df = hp_plot_df.sort_values(by = ['component', 'lambda_equilibrium', 'dataset'])\n",
    "hp_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in validation set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(hp_plot_df[(hp_plot_df.dataset == 'validation') & (hp_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(hp_plot_df[(hp_plot_df.dataset == 'validation') & (hp_plot_df.component == 'flow') ]['value'])\n",
    "z = hp_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = hp_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-20, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in training set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(hp_plot_df[(hp_plot_df.dataset == 'training') & (hp_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(hp_plot_df[(hp_plot_df.dataset == 'training') & (hp_plot_df.component == 'flow') ]['value'])\n",
    "z = hp_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = hp_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-25, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of TVGODLULPE with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntvgodlulpe: Time specific utility and generation, and link specific parameters for performance functions')\n",
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "models['tvgodlulpe'] = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods,\n",
    "                                                      historic_q = q_historic, features_Z = _FEATURES_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use optimal hyperparameter and do not run equilibrium stage\n",
    "_LOSS_WEIGHTS = optimal_weights\n",
    "_LOSS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_results_dfs['tvgodlulpe'], val_results_dfs['tvgodlulpe'] = models['tvgodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= _OPTIMIZERS,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    loss_weights= _LOSS_WEIGHTS,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    alternating_optimization=_ALTERNATING_OPTIMIZATION,\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvgodlulpe'].save_weights(models['tvgodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvgodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "print(generation_factors)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "growth_factor = 7.9/6.6\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(\n",
    "    q = network.q.flatten()[np.newaxis,:], ods= network.ods, n_nodes = len(network.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference\n",
    "inference_model = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods,\n",
    "                                                 historic_q = q_historic, features_Z = _FEATURES_Z)\n",
    "inference_model.build()\n",
    "inference_model.load_weights(models['tvgodlulpe']._filepath_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make prediction on 2020, the validation set, without computing equilibrium\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            evaluation_metric=_EVALUATION_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= _OPTIMIZERS['learning'],\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights= optimal_weights,\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC,\n",
    "                                                                     'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Comparison against data-driven top performing data-driven benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Link-level spatial information\n",
    "links_gdf['link_key'] = pd.Categorical(links_gdf['key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "\n",
    "# Create dataframe with data collected in 2020 during peak hours only\n",
    "model_df = df[(df.hour.isin([6,7,8, 15,16,17])) & (df['year']==2020)].sort_values(['period','link_key'])\n",
    "# links_gdf = links_gdf.sort_values(['link_key'])\n",
    "\n",
    "# Build dataset witg data collected between 4-5pm in the first Tuesdays of Oct 2019 and 2020\n",
    "benchmark_df = df[(df.hour == 16) & df['date'].isin(['2019-10-01', '2020-10-06'])].sort_values(['period','link_key'])\n",
    "\n",
    "fig_speed, fig_flow = plot_congestion_maps(model=inference_model, model_df=model_df, benchmark_df = benchmark_df,\n",
    "                     gdf=links_gdf.sort_values(['link_key']), features=_FEATURES_Z, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Global runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'runtime: {time.time()-t0_global:0.1f} [s]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nesuelogit",
   "language": "python",
   "name": "nesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
