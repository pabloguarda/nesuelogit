{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.colors as colors\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import random\n",
    "import isuelogit as isl\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/github/nesuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath(\"\")).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "sys.path.append(os.path.join(main_dir, 'src'))\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pesuelogit.networks import read_OD, load_k_shortest_paths\n",
    "from pesuelogit.etl import data_curation, add_period_id\n",
    "\n",
    "# Functions from internal modules\n",
    "from nesuelogit.models import compute_generated_trips, compute_generation_factors, \\\n",
    "    regularization_kfold, create_tvgodlulpe_model_fresno\n",
    "from nesuelogit.etl import build_network, get_tensors_by_year\n",
    "from nesuelogit.visualizations import plot_flow_vs_traveltime, plot_congestion_maps\n",
    "from nesuelogit.metrics import mse, mape, r2_score,  z2score, mdape\n",
    "from nesuelogit.utils import read_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report global runtime\n",
    "t0_global = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 231219120556\n"
     ]
    }
   ],
   "source": [
    "# Set timestamp to add in the filenames that are written in disk\n",
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "print('Timestamp:',ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Critical hyperparameters\n",
    "_EPOCHS = {'learning':10}\n",
    "_LR = {'learning': 1e-1, 'generation':10}\n",
    "_BATCH_SIZE = 1\n",
    "\n",
    "# Number of splits for k-fold method\n",
    "_N_SPLITS_HP = 5\n",
    "_GRID_EQUILIBRIUM_HP = [0, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "# _GRID_EQUILIBRIUM_HP = [0, 5e-1, 1, 2]\n",
    "# _GRID_EQUILIBRIUM_HP = [0, 5e-1]\n",
    "\n",
    "# These hyperparameters can be left in their current values\n",
    "_LOSS_WEIGHTS ={'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
    "_EQUILIBRIUM_STAGE = False\n",
    "_RELATIVE_GAP = float('inf')\n",
    "_LOSS_METRIC  = z2score\n",
    "_EVALUATION_METRIC = mdape\n",
    "_DTYPE = tf.float32\n",
    "_OPTIMIZERS = {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning'])}\n",
    "\n",
    "_DAYSOFWEEK = [1,2,3] # Monday:0, Sunday:6\n",
    "_HOURS = np.arange(6,21)\n",
    "#_HOURS = [6,7,8, 15,16,17]\n",
    "\n",
    "# Exogenous attributes in utility function\n",
    "_FEATURES_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read nodes and link-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv('./input/network-data/fresno/nodes/fresno-nodes-gis-data.csv')\n",
    "\n",
    "links_df = pd.read_csv('./input/network-data/fresno/links/fresno-link-specific-data.csv',\n",
    "                       converters={\"link_key\": ast.literal_eval, \"pems_id\": ast.literal_eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_network(links_df=links_df, nodes_df=nodes_df, crs='epsg:4326', key= 'fresno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Display network\n",
    "links_gdf = gpd.read_file('./input/network-data/fresno/gis/links/fresno-links-gis.shp').set_crs(\n",
    "        'EPSG:2228')\n",
    "ax = links_gdf.to_crs(epsg=3857).plot(figsize=(10, 10), alpha=0.5)\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.0[s] with sparse format\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n"
     ]
    }
   ],
   "source": [
    "read_OD(network=network, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read and load paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18289 paths were read and incidence matrices were built\n"
     ]
    }
   ],
   "source": [
    "# read_paths(network=network, update_incidence_matrices=True, filename = 'paths-fresno-k3.csv')\n",
    "read_paths(network=network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process spatio-temporal link-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = './input/network-data/fresno/links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year\n",
    "\n",
    "df['link_key'] = pd.Categorical(df['link_key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "df['period'] = pd.to_datetime(df['period'], format = '%Y-%m-%d-%H').dt.strftime('%Y-%m-%d-%H')\n",
    "\n",
    "# Select data from Tuesdays to Thursdays\n",
    "df = df[df['date'].dt.dayofweek.isin(_DAYSOFWEEK)]\n",
    "\n",
    "# Select data from first Tuesdays of 2019 and 2020\n",
    "# df = df[df['date'].isin([\"2019-10-01\", \"2020-10-06\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hour  period_id\n",
      "11     6          0\n",
      "12     7          1\n",
      "13     8          2\n",
      "14     9          3\n",
      "0     10          4\n",
      "1     11          5\n",
      "2     12          6\n",
      "3     13          7\n",
      "4     14          8\n",
      "5     15          9\n",
      "6     16         10\n",
      "7     17         11\n",
      "8     18         12\n",
      "9     19         13\n",
      "10    20         14\n"
     ]
    }
   ],
   "source": [
    "# Add period id for timevarying estimation\n",
    "period_feature = 'hour'\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data curation\n",
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd_adj']\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60\n",
    "\n",
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in network.links:\n",
    "    network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==link.key].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process node-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = nodes_df.rename(columns ={'pop_tract':'population','stops_tract': 'bus_stops','median_inc':'income'})\n",
    "\n",
    "features_generation = ['population','income', 'bus_stops']\n",
    "\n",
    "nodes_df = nodes_df[['key','type'] + features_generation]\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(nodes_df[features_generation])\n",
    "nodes_df[features_generation] = imp_mean.transform(nodes_df[features_generation])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(nodes_df[features_generation].values)\n",
    "nodes_df[features_generation] = scaler.transform(nodes_df[features_generation].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            speed_sd  speed_avg       counts  total_obs\ndate                                                   \n2019-10-01  1.731787  17.175187  1770.335035      36195\n2019-10-02  1.760109  17.169768  1746.651824      36195\n2019-10-03  1.754288  17.092304  1785.115209      36195\n2019-10-08  1.847060  18.165569  1747.732955      36195\n2019-10-09  1.917923  18.137042  1756.834846      36195\n2019-10-10  1.830232  18.107925  1793.512340      36195\n2019-10-15  1.831527  18.114384  1750.339155      36195\n2019-10-16  1.823680  18.162625  1760.170975      36195\n2019-10-17  1.832219  18.080860  1775.411385      36195\n2019-10-22  1.837839  18.175561  1738.314834      36195\n2019-10-23  1.864443  18.222690  1753.243283      36195\n2019-10-24  1.848827  18.184814  1746.626881      36195\n2019-10-29  1.868430  18.167171  1731.246241      36195\n2019-10-30  1.816364  18.109232  1757.581055      36195\n2019-10-31  1.882446  18.128430  1804.595108      36195\n2020-10-01  1.290377  19.579215  1616.266052      36195\n2020-10-06  1.365919  19.709670  1581.200095      36195\n2020-10-07  1.327017  19.927088  1587.425957      36195\n2020-10-08  1.344382  19.969943  1604.360618      36195\n2020-10-13  1.311842  19.626350  1607.663262      36195\n2020-10-14  1.300075  19.558364  1613.399054      36195\n2020-10-15  1.331493  19.621633  1637.123641      36195\n2020-10-20  1.353743  19.597736  1599.044113      36195\n2020-10-21  1.325694  19.501768  1587.235083      36195\n2020-10-22  1.334320  19.591921  1628.075697      36195\n2020-10-27  1.309348  19.573460  1583.478061      36195\n2020-10-28  1.324270  19.646064  1593.821986      36195\n2020-10-29  1.336212  19.557279  1627.548269      36195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speed_sd</th>\n      <th>speed_avg</th>\n      <th>counts</th>\n      <th>total_obs</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-01</th>\n      <td>1.731787</td>\n      <td>17.175187</td>\n      <td>1770.335035</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-02</th>\n      <td>1.760109</td>\n      <td>17.169768</td>\n      <td>1746.651824</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-03</th>\n      <td>1.754288</td>\n      <td>17.092304</td>\n      <td>1785.115209</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-08</th>\n      <td>1.847060</td>\n      <td>18.165569</td>\n      <td>1747.732955</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-09</th>\n      <td>1.917923</td>\n      <td>18.137042</td>\n      <td>1756.834846</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-10</th>\n      <td>1.830232</td>\n      <td>18.107925</td>\n      <td>1793.512340</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>1.831527</td>\n      <td>18.114384</td>\n      <td>1750.339155</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>1.823680</td>\n      <td>18.162625</td>\n      <td>1760.170975</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>1.832219</td>\n      <td>18.080860</td>\n      <td>1775.411385</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-22</th>\n      <td>1.837839</td>\n      <td>18.175561</td>\n      <td>1738.314834</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-23</th>\n      <td>1.864443</td>\n      <td>18.222690</td>\n      <td>1753.243283</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-24</th>\n      <td>1.848827</td>\n      <td>18.184814</td>\n      <td>1746.626881</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-29</th>\n      <td>1.868430</td>\n      <td>18.167171</td>\n      <td>1731.246241</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-30</th>\n      <td>1.816364</td>\n      <td>18.109232</td>\n      <td>1757.581055</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2019-10-31</th>\n      <td>1.882446</td>\n      <td>18.128430</td>\n      <td>1804.595108</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-01</th>\n      <td>1.290377</td>\n      <td>19.579215</td>\n      <td>1616.266052</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-06</th>\n      <td>1.365919</td>\n      <td>19.709670</td>\n      <td>1581.200095</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-07</th>\n      <td>1.327017</td>\n      <td>19.927088</td>\n      <td>1587.425957</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-08</th>\n      <td>1.344382</td>\n      <td>19.969943</td>\n      <td>1604.360618</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-13</th>\n      <td>1.311842</td>\n      <td>19.626350</td>\n      <td>1607.663262</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-14</th>\n      <td>1.300075</td>\n      <td>19.558364</td>\n      <td>1613.399054</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-15</th>\n      <td>1.331493</td>\n      <td>19.621633</td>\n      <td>1637.123641</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-20</th>\n      <td>1.353743</td>\n      <td>19.597736</td>\n      <td>1599.044113</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-21</th>\n      <td>1.325694</td>\n      <td>19.501768</td>\n      <td>1587.235083</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-22</th>\n      <td>1.334320</td>\n      <td>19.591921</td>\n      <td>1628.075697</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-27</th>\n      <td>1.309348</td>\n      <td>19.573460</td>\n      <td>1583.478061</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-28</th>\n      <td>1.324270</td>\n      <td>19.646064</td>\n      <td>1593.821986</td>\n      <td>36195</td>\n    </tr>\n    <tr>\n      <th>2020-10-29</th>\n      <td>1.336212</td>\n      <td>19.557279</td>\n      <td>1627.548269</td>\n      <td>36195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "              tt_sd    median_inc     incidents     bus_stops  intersections\ncount  1.013460e+06  1.013460e+06  1.013460e+06  1.013460e+06   1.013460e+06\nmean   1.813680e-02  2.621913e+01  7.441093e-01  1.500207e-01   8.765023e-01\nstd    2.783112e-02  2.135738e+01  3.193143e+00  4.411927e-01   1.319496e+00\nmin    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00\n25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00\n50%    1.113761e-02  2.482100e+01  0.000000e+00  0.000000e+00   0.000000e+00\n75%    2.548870e-02  4.168100e+01  0.000000e+00  0.000000e+00   1.000000e+00\nmax    7.459602e-01  1.158930e+02  4.000000e+01  4.000000e+00   9.000000e+00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tt_sd</th>\n      <th>median_inc</th>\n      <th>incidents</th>\n      <th>bus_stops</th>\n      <th>intersections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n      <td>1.013460e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.813680e-02</td>\n      <td>2.621913e+01</td>\n      <td>7.441093e-01</td>\n      <td>1.500207e-01</td>\n      <td>8.765023e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.783112e-02</td>\n      <td>2.135738e+01</td>\n      <td>3.193143e+00</td>\n      <td>4.411927e-01</td>\n      <td>1.319496e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.113761e-02</td>\n      <td>2.482100e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.548870e-02</td>\n      <td>4.168100e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.459602e-01</td>\n      <td>1.158930e+02</td>\n      <td>4.000000e+01</td>\n      <td>4.000000e+00</td>\n      <td>9.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Link-level attributes in utility function\n",
    "df[_FEATURES_Z].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "XT, YT = get_tensors_by_year(df[df.hour.isin(_HOURS)], features_Z = _FEATURES_Z, links_keys=list(network.links_dict.keys()))\n",
    "\n",
    "# Split in training and test sets\n",
    "XT_train, XT_val, YT_train, YT_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [XT[2019], XT[2020], YT[2019], YT[2020]])\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reference O-D matrix and trip generation vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "q_historic = np.repeat(network.q.flatten()[np.newaxis, :], len(_HOURS), axis=0)\n",
    "\n",
    "#Adjust historic O-D and historic trip generation\n",
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                              flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "reference_q = q_historic*np.tile(generation_factors.values,(q_historic.shape[1],1)).T\n",
    "\n",
    "reference_g = compute_generated_trips(q = reference_q, ods= network.ods, n_nodes = len(network.nodes))\n",
    "\n",
    "# Total trips tvodlulpe pesuelogit:\n",
    "# Epoch 0: 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04 6.6e+04\n",
    "# Final epoch: 6.4e+04 6.6e+04 6.3e+04 7.8e+04 7.9e+04 7.9e+04\n",
    "# Growth factor captures the difference between the reference OD at epoch 0 and the estimated OD in Guarda et al., (2024), Transportation Research Part C\n",
    "# growth_factor = 7.9/6.6\n",
    "# reference_g = growth_factor*reference_g\n",
    "# reference_q = growth_factor*reference_q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "val_results_dfs = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Search of optimal hyperparameter weighting the equilibrium component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "target_metric = 'mse'\n",
    "target_component = 'flow'\n",
    "\n",
    "loss_weights = []\n",
    "\n",
    "if isinstance(_GRID_EQUILIBRIUM_HP, (int, float)):\n",
    "    _GRID_EQUILIBRIUM_HP = [_GRID_EQUILIBRIUM_HP]\n",
    "\n",
    "for i in _GRID_EQUILIBRIUM_HP:\n",
    "    loss_weights.append(_LOSS_WEIGHTS.copy())\n",
    "    loss_weights[-1]['equilibrium'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replicate: 1/6\n",
      "\n",
      "weights:  {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 12:16:11.641500: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "Model training\n",
      "\n",
      "Pretraining generation weights\n",
      "\n",
      "period 0 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 1 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 2 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 3 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 4 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 5 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 6 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 7 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 8 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 9 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 10 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 11 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 12 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 13 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 14 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "\n",
      "Link flows and travel times were pretrained with single pass of traffic assignment\n",
      "Benchmark metrics using historical mean in training data to make predictions in the validation set: \n",
      "\n",
      "\n",
      "    component   value metric\n",
      "0        flow      27  mdape\n",
      "1  traveltime      53  mdape\n",
      "0        flow 7.1e+05    mse\n",
      "1  traveltime   0.072    mse\n",
      "0        flow  -0.076     r2\n",
      "1  traveltime -0.0021     r2\n",
      "\n",
      "hyperparameters loss function: {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 0}\n",
      "\n",
      "number of periods: 15, batch size: 1, threshold relative gap: inf\n",
      "training set -> timepoints: 225, obs [t x]: [1108.  111.], coverage [t x]: [0.46 0.05]\n",
      "validation set -> timepoints: 225, obs [t x]: [277.  29.], coverage [t x]: [0.11 0.01]\n",
      "\n",
      "Learning stage: 10 epochs\n",
      "\n",
      "0/11: train mse=1.1e+06, train mse traveltime=8.7, train mdape traveltime=38.6 train mse flow=2.2e+06, train mdape flow=62.3, val mse=1.6e+06, val mse traveltime=1.4, val mdape traveltime=43.3 val mse flow=3.1e+06, val mdape flow=72.0, avg theta = [-3.06  -3.268  0.    -4.537  0.    -3.879], avg rr = 1.07, avg theta fixed effect = 0, loss prop od=1.3e+03, total trips=[5.46e+04 6.28e+04 5.47e+04 4.91e+04 4.92e+04 5.16e+04 5.40e+04 5.76e+04\n",
      " 6.28e+04 6.63e+04 6.63e+04 5.87e+04 4.56e+04 3.58e+04 2.98e+04], polynomial weights: [1.0e+00 1.0e+00 1.0e+00], kappa = [107.204  23.066 -31.424], lambda eq=0, relative gap=0.044, train equilibrium loss=2.1e+04, val equilibrium loss=2.1e+04, time: 12.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m create_tvgodlulpe_model_fresno(network \u001B[38;5;241m=\u001B[39m network, n_periods \u001B[38;5;241m=\u001B[39m n_periods, features_Z \u001B[38;5;241m=\u001B[39m _FEATURES_Z,\n\u001B[1;32m      2\u001B[0m                                        historic_g \u001B[38;5;241m=\u001B[39m reference_g, historic_q \u001B[38;5;241m=\u001B[39m reference_q)\n\u001B[1;32m      4\u001B[0m hp_metrics_df, optimal_weights, optimal_metrics_kfold_df, optimal_parameters_kfold_df \\\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;241m=\u001B[39m \u001B[43mregularization_kfold\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_metric\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmse\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_component\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mflow\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_N_SPLITS_HP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_SEED\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mXT_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mYT_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_OPTIMIZERS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnode_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_LOSS_METRIC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_EVALUATION_METRIC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs_print_interval\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthreshold_relative_gap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_RELATIVE_GAP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_BATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:2986\u001B[0m, in \u001B[0;36mregularization_kfold\u001B[0;34m(loss_weights, target_metric, target_component, **kwargs)\u001B[0m\n\u001B[1;32m   2983\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mReplicate: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(loss_weights))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2984\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweights:  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweights\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2986\u001B[0m metrics_kfold_df, parameters_kfold_df \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_kfold\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2988\u001B[0m \u001B[38;5;66;03m# Stores metrics\u001B[39;00m\n\u001B[1;32m   2989\u001B[0m metrics_kfold_dfs\u001B[38;5;241m.\u001B[39mappend(metrics_kfold_df)\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:2891\u001B[0m, in \u001B[0;36mtrain_kfold\u001B[0;34m(model, X, Y, n_splits, random_state, evaluation_metric, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2888\u001B[0m \u001B[38;5;66;03m# Compute metrics_df after training\u001B[39;00m\n\u001B[1;32m   2889\u001B[0m model\u001B[38;5;241m.\u001B[39mload_weights(filepath)\n\u001B[0;32m-> 2891\u001B[0m train_results_df, val_results_df \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2892\u001B[0m relative_gap \u001B[38;5;241m=\u001B[39m train_results_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelative_gap\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   2894\u001B[0m \u001B[38;5;66;03m# Store parameters values\u001B[39;00m\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:2577\u001B[0m, in \u001B[0;36mNESUELOGIT.fit\u001B[0;34m(self, X_train, Y_train, X_val, Y_val, node_data, optimizers, loss_weights, epochs, threshold_relative_gap, loss_metric, evaluation_metric, momentum_equilibrium, pretrain_link_flows, equilibrium_stage, alternating_optimization, relative_losses, epochs_print_interval, batch_size)\u001B[0m\n\u001B[1;32m   2574\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, (X_batch_train, Y_batch_train) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataset):\n\u001B[1;32m   2575\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m   2576\u001B[0m         train_loss \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m-> 2577\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_batch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY_batch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambdas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2578\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mloss_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_metric\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_total\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m   2579\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeneration\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m optimizers\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m   2580\u001B[0m         optimizers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeneration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m optimizers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:1273\u001B[0m, in \u001B[0;36mNESUELOGIT.loss_function\u001B[0;34m(self, X, Y, lambdas, loss_metric, epoch)\u001B[0m\n\u001B[1;32m   1259\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss_function\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1260\u001B[0m                   X,\n\u001B[1;32m   1261\u001B[0m                   Y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1264\u001B[0m                   epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1265\u001B[0m                   ):\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1267\u001B[0m \u001B[38;5;124;03m    Return a dictionary with keys defined as the different terms of the loss function\u001B[39;00m\n\u001B[1;32m   1268\u001B[0m \n\u001B[1;32m   1269\u001B[0m \u001B[38;5;124;03m    # loss_metric = btcg_mse, mse\u001B[39;00m\n\u001B[1;32m   1270\u001B[0m \n\u001B[1;32m   1271\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1273\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1275\u001B[0m     \u001B[38;5;66;03m# Normalize weights to one\u001B[39;00m\n\u001B[1;32m   1276\u001B[0m     lambdas \u001B[38;5;241m=\u001B[39m {k: (v \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;28mlist\u001B[39m(lambdas\u001B[38;5;241m.\u001B[39mvalues()))) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m lambdas\u001B[38;5;241m.\u001B[39mitems()}\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:1138\u001B[0m, in \u001B[0;36mNESUELOGIT.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod_ids \u001B[38;5;241m=\u001B[39m X[:, :, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1133\u001B[0m \u001B[38;5;66;03m# predicted_flow = self.compute_link_flows(X)\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;66;03m# output_flow = predicted_flow\u001B[39;00m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;66;03m# predicted_traveltimes = self.bpr_traveltimes(predicted_flow)\u001B[39;00m\n\u001B[1;32m   1136\u001B[0m \n\u001B[1;32m   1137\u001B[0m \u001B[38;5;66;03m# input_flow = tf.stop_gradient(self.flows)\u001B[39;00m\n\u001B[0;32m-> 1138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_flow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_flow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;66;03m# self._predicted_flow = self.input_flow  # tf.stop_gradient(self.predict_flow())\u001B[39;00m\n\u001B[1;32m   1141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predicted_flow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict_flow(training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:2071\u001B[0m, in \u001B[0;36mNESUELOGIT.output_flow\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   2069\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_flow\n\u001B[1;32m   2070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2071\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_link_flows\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py:2082\u001B[0m, in \u001B[0;36mNESUELOGIT.compute_link_flows\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   2074\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2075\u001B[0m \u001B[38;5;124;03mCompute output link flows\u001B[39;00m\n\u001B[1;32m   2076\u001B[0m \u001B[38;5;124;03m:param X: Matrix with link level features\u001B[39;00m\n\u001B[1;32m   2077\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[1;32m   2078\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2080\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod_ids \u001B[38;5;241m=\u001B[39m X[:, :, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m-> 2082\u001B[0m link_flows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlink_flows\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2083\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_flows\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_probabilities\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_utilities\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlink_utilities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2085\u001B[0m \u001B[38;5;66;03m# if tf.rank(link_flows) == 2:\u001B[39;00m\n\u001B[1;32m   2086\u001B[0m \u001B[38;5;66;03m#     return self.split_link_flows_period(link_flows)\u001B[39;00m\n\u001B[1;32m   2087\u001B[0m \n\u001B[1;32m   2088\u001B[0m \u001B[38;5;66;03m# return tf.reduce_mean(link_flows,axis = 0)\u001B[39;00m\n\u001B[1;32m   2090\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m link_flows\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/pesuelogit/models.py:772\u001B[0m, in \u001B[0;36mPESUELOGIT.link_flows\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m    771\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlink_flows\u001B[39m(\u001B[38;5;28mself\u001B[39m, f):\n\u001B[0;32m--> 772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjk,lk -> jl\u001B[39m\u001B[38;5;124m\"\u001B[39m, f, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mD\u001B[49m)\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/pesuelogit/models.py:591\u001B[0m, in \u001B[0;36mPESUELOGIT.D\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mD\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001B[0m, in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconstant\u001B[39m(value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConst\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    172\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mallow_broadcast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001B[0m, in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.constant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    278\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001B[0;32m--> 279\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_eager_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mget_default_graph()\n\u001B[1;32m    282\u001B[0m tensor_value \u001B[38;5;241m=\u001B[39m attr_value_pb2\u001B[38;5;241m.\u001B[39mAttrValue()\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001B[0m, in \u001B[0;36m_constant_eager_impl\u001B[0;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_eager_impl\u001B[39m(ctx, value, dtype, shape, verify_shape):\n\u001B[1;32m    303\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m   t \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_eager_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "File \u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods, features_Z = _FEATURES_Z,\n",
    "                                       historic_g = reference_g, historic_q = reference_q)\n",
    "\n",
    "hp_metrics_df, optimal_weights, optimal_metrics_kfold_df, optimal_parameters_kfold_df \\\n",
    "    = regularization_kfold(\n",
    "    loss_weights=loss_weights,\n",
    "    target_metric = 'mse',\n",
    "    target_component = 'flow',\n",
    "    n_splits=_N_SPLITS_HP,\n",
    "    random_state=_SEED,\n",
    "    model=model,\n",
    "    X=XT_train, Y=YT_train,\n",
    "    optimizers=_OPTIMIZERS,\n",
    "    node_data=nodes_df,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    epochs_print_interval = _EPOCHS,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    epochs=_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filepath = f\"output/tables/{ts}_hyperparameter_tuning_{'fresno'}.csv\"\n",
    "hp_metrics_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hp_plot_df = pd.read_csv(filepath)\n",
    "hp_plot_df = hp_plot_df.sort_values(by = ['component', 'lambda_equilibrium', 'dataset'])\n",
    "hp_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in validation set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(hp_plot_df[(hp_plot_df.dataset == 'validation') & (hp_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(hp_plot_df[(hp_plot_df.dataset == 'validation') & (hp_plot_df.component == 'flow') ]['value'])\n",
    "z = hp_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = hp_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-20, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Losses in training set\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize = (12,6))\n",
    "\n",
    "x = np.log10(hp_plot_df[(hp_plot_df.dataset == 'training') & (hp_plot_df.component == 'traveltime') ]['value'])\n",
    "y = np.log10(hp_plot_df[(hp_plot_df.dataset == 'training') & (hp_plot_df.component == 'flow') ]['value'])\n",
    "z = hp_plot_df['lambda_equilibrium'].sort_values().unique()\n",
    "\n",
    "c = hp_plot_df[['lambda_equilibrium', 'relative_gap']].sort_values(['lambda_equilibrium'])['relative_gap'].drop_duplicates().values\n",
    "\n",
    "p = ax.scatter(x,y,z,\n",
    "               c =c,\n",
    "               # c =np.log10(hyperparameter_search_eq['loss_eq']),\n",
    "               norm=colors.LogNorm(vmin=1e-2, vmax=6e-2),\n",
    "               s=40, cmap='Blues_r')\n",
    "\n",
    "cbar = plt.colorbar(p,\n",
    "                    #ticks=[1e-3,1e-4,1e-5,1e-6,1e-7],\n",
    "                    #ticks=np.linspace(start = 1e-6, stop = 1e-7,num = 5),\n",
    "                    cax = fig.add_axes([0.78, 0.28, 0.03, 0.38]))\n",
    "\n",
    "ax.set_xlabel(r'$\\log(\\ell_t)$')\n",
    "ax.set_ylabel(r'$\\log(\\ell_x)$')\n",
    "ax.set_zlabel(r'$\\lambda_{e}$')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax.view_init(elev=10., azim=-25, roll=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of TVGODLULPE with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tvgodlulpe: Time specific utility and generation, and link specific parameters for performance functions\n"
     ]
    }
   ],
   "source": [
    "print('\\ntvgodlulpe: Time specific utility and generation, and link specific parameters for performance functions')\n",
    "\n",
    "# To report runtime\n",
    "t0 = time.time()\n",
    "\n",
    "models['tvgodlulpe'] = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods, features_Z = _FEATURES_Z,\n",
    "                                                      historic_g = reference_g, historic_q = reference_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Use optimal hyperparameter and do not run equilibrium stage\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m _LOSS_WEIGHTS \u001B[38;5;241m=\u001B[39m \u001B[43moptimal_weights\u001B[49m\n\u001B[1;32m      3\u001B[0m _LOSS_WEIGHTS\n",
      "\u001B[0;31mNameError\u001B[0m: name 'optimal_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Use optimal hyperparameter and do not run equilibrium stage\n",
    "_LOSS_WEIGHTS = optimal_weights.copy()\n",
    "optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training\n",
      "\n",
      "Pretraining generation weights\n",
      "\n",
      "period 0 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 1 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 2 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 3 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 4 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 5 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 6 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 7 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 8 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 9 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 10 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 11 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 12 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 13 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "period 14 -> p-values kappa: {'population': 0.0017, 'income': 0.5592, 'bus_stops': 0.341}\n",
      "\n",
      "Link flows and travel times were pretrained with single pass of traffic assignment\n",
      "Benchmark metrics using historical mean in training data to make predictions in the validation set: \n",
      "\n",
      "\n",
      "    component   value metric\n",
      "0        flow      30  mdape\n",
      "1  traveltime      40  mdape\n",
      "0        flow 5.8e+05    mse\n",
      "1  traveltime   0.066    mse\n",
      "0        flow  -0.044     r2\n",
      "1  traveltime -0.0004     r2\n",
      "\n",
      "hyperparameters loss function: {'od': 0, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
      "\n",
      "number of periods: 15, batch size: 1, threshold relative gap: inf\n",
      "training set -> timepoints: 225, obs [t x]: [2081.  140.], coverage [t x]: [0.86 0.06]\n",
      "validation set -> timepoints: 195, obs [t x]: [2151.  140.], coverage [t x]: [0.89 0.06]\n",
      "\n",
      "Learning stage: 10 epochs\n",
      "\n",
      "0/11: train mse=8.1e+05, train mse traveltime=7.2, train mdape traveltime=39.4 train mse flow=2.4e+06, train mdape flow=64.5, val mse=8.7e+05, val mse traveltime=6.9, val mdape traveltime=41.4 val mse flow=2.6e+06, val mdape flow=64.3, avg theta = [-3.06  -3.268  0.    -4.537  0.    -3.879], avg rr = 1.07, avg theta fixed effect = 0, loss prop od=1.3e+03, total trips=[5.46e+04 6.28e+04 5.47e+04 4.91e+04 4.92e+04 5.16e+04 5.40e+04 5.76e+04\n",
      " 6.28e+04 6.63e+04 6.63e+04 5.87e+04 4.56e+04 3.58e+04 2.98e+04], polynomial weights: [1.0e+00 1.0e+00 1.0e+00], kappa = [107.204  23.066 -31.424], lambda eq=1, relative gap=0.044, train equilibrium loss=2.1e+04, val equilibrium loss=1e+04, time: 10.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/mg/t5yt0b_d1cg4ykytg0c22xcc0000gn/T/ipykernel_70332/1042710001.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m train_results_dfs['tvgodlulpe'], val_results_dfs['tvgodlulpe'] = models['tvgodlulpe'].fit(\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mXT_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mYT_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXT_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mYT_val\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mnode_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnodes_df\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0moptimizers\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0m_OPTIMIZERS\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X_train, Y_train, X_val, Y_val, node_data, optimizers, loss_weights, epochs, threshold_relative_gap, loss_metric, evaluation_metric, momentum_equilibrium, pretrain_link_flows, equilibrium_stage, alternating_optimization, relative_losses, epochs_print_interval, batch_size)\u001B[0m\n\u001B[1;32m   2573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2574\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mX_batch_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY_batch_train\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2575\u001B[0m                         \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2576\u001B[0m                             \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2577\u001B[0;31m                                 self.loss_function(X=X_batch_train, Y=Y_batch_train, lambdas=loss_weights,\n\u001B[0m\u001B[1;32m   2578\u001B[0m                                                    loss_metric=loss_metric)['loss_total']\n\u001B[1;32m   2579\u001B[0m                         \u001B[0;32mif\u001B[0m \u001B[0;34m'generation'\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2580\u001B[0m                             \u001B[0moptimizers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'generation'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'learning'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, Y, lambdas, loss_metric, epoch)\u001B[0m\n\u001B[1;32m   1269\u001B[0m         \u001B[0;31m# loss_metric = btcg_mse, mse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1271\u001B[0m         \"\"\"\n\u001B[1;32m   1272\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1273\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1274\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1275\u001B[0m         \u001B[0;31m# Normalize weights to one\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1276\u001B[0m         \u001B[0mlambdas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlambdas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlambdas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   1134\u001B[0m         \u001B[0;31m# output_flow = predicted_flow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1135\u001B[0m         \u001B[0;31m# predicted_traveltimes = self.bpr_traveltimes(predicted_flow)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1137\u001B[0m         \u001B[0;31m# input_flow = tf.stop_gradient(self.flows)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1138\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output_flow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_flow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1139\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1140\u001B[0m         \u001B[0;31m# self._predicted_flow = self.input_flow  # tf.stop_gradient(self.predict_flow())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predicted_flow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_flow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   2067\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2068\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mX\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2069\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output_flow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2070\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2071\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_link_flows\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   2079\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2080\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperiod_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2082\u001B[0m         link_flows = self.link_flows(\n\u001B[0;32m-> 2083\u001B[0;31m             self.path_flows(self.path_probabilities(self.path_utilities(self.link_utilities(X)))))\n\u001B[0m\u001B[1;32m   2084\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2085\u001B[0m         \u001B[0;31m# if tf.rank(link_flows) == 2:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2086\u001B[0m         \u001B[0;31m#     return self.split_link_flows_period(link_flows)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/pesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, vf, sparse_mode, normalization)\u001B[0m\n\u001B[1;32m    741\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpath_probabilities\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msparse_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalization\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    742\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    743\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msparse_mode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 744\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath_probabilities_sparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalization\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnormalization\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    745\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m         \u001B[0;31m# TODO: Readapt this code to account for the hurs dimension or simply remove this as it is not used by default\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    747\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnormalization\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/github/nesuelogit/src/nesuelogit/models.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, vf, normalization)\u001B[0m\n\u001B[1;32m   1596\u001B[0m         \u001B[0mhttps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0mwww\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morg\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mapi_docs\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mstop_gradient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1597\u001B[0m         \u001B[0;31m#TODO: Optimize repetition of M_sparse matrix over days and hours dimensions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1598\u001B[0m         '''\n\u001B[1;32m   1599\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1600\u001B[0;31m         \u001B[0mM_sparse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_dense\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mM\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1601\u001B[0m         \u001B[0;31m# M_sparse = tf.sparse.concat(0, [tf.sparse.expand_dims(M_sparse, 0)] * vf.shape[1])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1602\u001B[0m         \u001B[0mM_sparse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mM_sparse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mvf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/ops/sparse_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(tensor, name)\u001B[0m\n\u001B[1;32m    124\u001B[0m   \"\"\"\n\u001B[1;32m    125\u001B[0m   \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"dense_to_sparse\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m     indices = array_ops.where_v2(\n\u001B[0;32m--> 128\u001B[0;31m         math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n\u001B[0m\u001B[1;32m    129\u001B[0m     \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather_nd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m     \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msparse_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSparseTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1079\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1080\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1082\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1083\u001B[0;31m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1084\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1085\u001B[0m         \u001B[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1086\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   1955\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1956\u001B[0m   \u001B[0mRaises\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1957\u001B[0m     \u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIf\u001B[0m \u001B[0mshapes\u001B[0m \u001B[0mof\u001B[0m \u001B[0marguments\u001B[0m \u001B[0mare\u001B[0m \u001B[0mincompatible\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1958\u001B[0m   \"\"\"\n\u001B[0;32m-> 1959\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnot_equal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniforge3/envs/nesuelogit/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(x, y, incompatible_shape_error, name)\u001B[0m\n\u001B[1;32m   6927\u001B[0m         incompatible_shape_error)\n\u001B[1;32m   6928\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6929\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6930\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6931\u001B[0;31m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6932\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6933\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6934\u001B[0m       return not_equal_eager_fallback(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_results_dfs['tvgodlulpe'], val_results_dfs['tvgodlulpe'] = models['tvgodlulpe'].fit(\n",
    "    XT_train, YT_train, XT_val, YT_val,\n",
    "    node_data=nodes_df,\n",
    "    optimizers= _OPTIMIZERS,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    loss_weights= _LOSS_WEIGHTS,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE,\n",
    "    threshold_relative_gap=_RELATIVE_GAP,\n",
    "    epochs=_EPOCHS)\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')\n",
    "\n",
    "# Save model weights for prediction analyses\n",
    "models['tvgodlulpe'].save_weights(models['tvgodlulpe']._filepath_weights)\n",
    "print(f\"\\nModel weights were saved at '{models['tvgodlulpe']._filepath_weights}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "print(generation_factors)\n",
    "\n",
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "growth_factor = 7.9/6.6\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*compute_generated_trips(\n",
    "    q = network.q.flatten()[np.newaxis,:], ods= network.ods, n_nodes = len(network.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model for inference\n",
    "inference_model = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods, features_Z = _FEATURES_Z,\n",
    "                                                 historic_g = generated_trips, historic_q = reference_q)\n",
    "inference_model.build()\n",
    "inference_model.load_weights(models['tvgodlulpe']._filepath_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make prediction on 2020, the validation set, without computing equilibrium\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            evaluation_metric=_EVALUATION_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= _OPTIMIZERS['learning'],\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights= optimal_weights,\n",
    "                            threshold_relative_gap=_RELATIVE_GAP,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC,\n",
    "                                                                     'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Comparison against data-driven top performing data-driven benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Link-level spatial information\n",
    "links_gdf['link_key'] = pd.Categorical(links_gdf['key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "\n",
    "# Create dataframe with data collected in 2020 during peak hours only\n",
    "model_df = df[(df.hour.isin(_HOURS)) & (df['year']==2020)].sort_values(['period','link_key'])\n",
    "# links_gdf = links_gdf.sort_values(['link_key'])\n",
    "\n",
    "# Build dataset witg data collected between 4-5pm in the first Tuesdays of Oct 2019 and 2020\n",
    "benchmark_df = df[(df.hour == 16) & df['date'].isin(['2019-10-01', '2020-10-06'])].sort_values(['period','link_key'])\n",
    "\n",
    "fig_speed, fig_flow = plot_congestion_maps(model=inference_model, model_df=model_df, benchmark_df = benchmark_df,\n",
    "                     gdf=links_gdf.sort_values(['link_key']), features=_FEATURES_Z, cmap = 'viridis')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Global runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'runtime: {time.time()-t0_global:0.1f} [s]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nesuelogit",
   "language": "python",
   "name": "nesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
