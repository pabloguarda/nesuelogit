{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import random\n",
    "import isuelogit as isl\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath(\"\")).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "sys.path.append(os.path.join(main_dir, 'src'))\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pesuelogit.networks import read_OD, load_k_shortest_paths\n",
    "from pesuelogit.etl import data_curation, add_period_id\n",
    "\n",
    "# Internal modules\n",
    "from nesuelogit.models import create_tvodlulpe_model_fresno, compute_generated_trips, compute_generation_factors, \\\n",
    "    create_tvgodlulpe_model_fresno, compute_baseline_predictions_kfold, train_kfold\n",
    "from nesuelogit.etl import build_network, get_tensors_by_year\n",
    "from nesuelogit.visualizations import plot_flow_vs_traveltime, plot_congestion_maps, \\\n",
    "    plot_metrics_kfold, plot_parameters_kfold, plot_baselines_kfold\n",
    "from nesuelogit.metrics import mse, mape, r2_score,  z2score, mdape\n",
    "from nesuelogit.utils import read_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report global runtime\n",
    "t0_global = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set timestamp to add in the filenames that are written in disk\n",
    "ts = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "print('Timestamp:',ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization in k-fold\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}\n",
    "\n",
    "_EQUILIBRIUM_STAGE = {'tvodlulpe': True, 'tvgodlulpe': False}\n",
    "_LR = {'learning': 1e-1, 'equilibrium': 5e-2}\n",
    "_RELATIVE_GAP = 1e-4\n",
    "_BATCH_SIZE = 1\n",
    "_EPOCHS = {'tvodlulpe':{'learning': 30, 'equilibrium': 30},\n",
    "           'tvgodlulpe':{'learning': 30, 'equilibrium': 30}}\n",
    "# _EPOCHS = {'tvodlulpe':{'learning': 3, 'equilibrium': 3},\n",
    "#            'tvgodlulpe':{'learning': 3, 'equilibrium': 3}}\n",
    "# _BATCH_SIZE = None\n",
    "# _RELATIVE_GAP = float('inf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_LOSS_METRIC  = z2score\n",
    "_EVALUATION_METRIC = mdape\n",
    "_OPTIMIZERS = {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "              'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "              }\n",
    "\n",
    "model_filepaths = {'tvodlulpe':'output/models/231212010520_fresno_tvodlulpe.h5',\n",
    "                   'tvgodlulpe':'output/models/231212010520_fresno_tvgodlulpe.h5'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Read nodes and link-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv('./input/network-data/fresno/nodes/fresno-nodes-gis-data.csv')\n",
    "\n",
    "links_df = pd.read_csv('./input/network-data/fresno/links/fresno-link-specific-data.csv',\n",
    "                       converters={\"link_key\": ast.literal_eval, \"pems_id\": ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Display network\n",
    "links_gdf = gpd.read_file('./input/network-data/fresno/gis/links/fresno-links-gis.shp').set_crs(\n",
    "        'EPSG:2228')\n",
    "ax = links_gdf.to_crs(epsg=3857).plot(figsize=(10, 10), alpha=0.5)\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_network(links_df=links_df, nodes_df=nodes_df, crs='epsg:4326', key= 'fresno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_OD(network=network, sparse=True)\n",
    "\n",
    "q_historic = np.repeat(network.q.flatten()[np.newaxis, :], 6, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_paths(network=network, update_incidence_matrices=True, filename = 'paths-fresno-k3.csv')\n",
    "read_paths(network=network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = './input/network-data/fresno/links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "df['link_key'] = pd.Categorical(df['link_key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "df['period'] = pd.to_datetime(df['period'], format = '%Y-%m-%d-%H').dt.strftime('%Y-%m-%d-%H')\n",
    "\n",
    "# Select data from Tuesdays to Thursdays\n",
    "df = df[df['date'].dt.dayofweek.between(1, 3)]\n",
    "\n",
    "# # Select data from first Tuesdays of 2019 and 2020\n",
    "# df = df[df['date'].isin([\"2019-10-01\", \"2020-10-06\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add period id for timevarying estimation\n",
    "period_feature = 'hour'\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)\n",
    "\n",
    "df['tt_sd'] = df['tt_sd_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = nodes_df.rename(columns ={'pop_tract':'population','stops_tract': 'bus_stops','median_inc':'income'})\n",
    "\n",
    "features_generation = ['population','income', 'bus_stops']\n",
    "\n",
    "nodes_df = nodes_df[['key','type'] + features_generation]\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(nodes_df[features_generation])\n",
    "nodes_df[features_generation] = imp_mean.transform(nodes_df[features_generation])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(nodes_df[features_generation].values)\n",
    "nodes_df[features_generation] = scaler.transform(nodes_df[features_generation].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURES_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in network.links:\n",
    "    network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==link.key].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[_FEATURES_Z].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DTYPE = tf.float32\n",
    "\n",
    "X, Y = {}, {}\n",
    "\n",
    "# Hourly data DURING morning and afternoon peak hour windows (6 hour intervals) to estimate TVODLULPE\n",
    "XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8, 15,16,17])], features_Z = _FEATURES_Z, links_keys=list(network.links_dict.keys()))\n",
    "\n",
    "# Split in training and test sets\n",
    "XT_train, XT_val, YT_train, YT_val = map(lambda x: tf.cast(x, dtype = _DTYPE), [XT[2019], XT[2020], YT[2019], YT[2020]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "val_results_dfs = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forecasting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))\n",
    "\n",
    "# Growth factor captures the difference between the reference OD at epoch 0 and the estimated OD.\n",
    "growth_factor = 7.9/6.6 # 1\n",
    "\n",
    "generation_factors = compute_generation_factors(period_column=XT_train[:, :, -1, None].numpy(),\n",
    "                                                              flow_column=YT_train[:,:,1, None].numpy(), reference_period=10)\n",
    "\n",
    "generated_trips = growth_factor*generation_factors.values[:,np.newaxis]*\\\n",
    "                  compute_generated_trips(q = q_historic, ods= network.ods, n_nodes = len(network.nodes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TVODLULPE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model = create_tvodlulpe_model_fresno(network = network, n_periods = n_periods, historic_q = q_historic,\n",
    "                                                features_Z = _FEATURES_Z, dtype = _DTYPE)\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inference_model.load_weights(model_filepaths['tvodlulpe'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Model estimated with all data from 2019"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= _BATCH_SIZE,\n",
    "                        optimizer= _OPTIMIZERS['equilibrium'],\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        threshold_relative_gap=_RELATIVE_GAP,\n",
    "                        epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={_EVALUATION_METRIC.__name__: _EVALUATION_METRIC, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC,\n",
    "                                                                         'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=_RELATIVE_GAP,\n",
    "                            epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TVGODLULPE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create model for inference (make sure that the arguments are the same than those used to create the reference model)\n",
    "inference_model = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods, features_Z = _FEATURES_Z,\n",
    "                                                 historic_g = generated_trips, historic_q = q_historic)\n",
    "inference_model.build()\n",
    "#inference_model.update_predictions(XT_train, update_period_dict = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Find a right model\n",
    "inference_model.load_weights(model_filepaths['tvgodlulpe'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Model estimated with all data from 2019"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reference_model.load_node_data(node_data)\n",
    "# This should convergence immediately when the data for prediction is equal to training set\n",
    "_ = inference_model.predict(XT_train,\n",
    "                        # period_dict = reference_model.period_dict,\n",
    "                        node_data=nodes_df,\n",
    "                        loss_metric=_LOSS_METRIC,\n",
    "                        pretrain_link_flows = False,\n",
    "                        batch_size= _BATCH_SIZE,\n",
    "                        optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                        loss_weights={'equilibrium': 1},\n",
    "                        threshold_relative_gap=float('inf'), #_RELATIVE_GAP\n",
    "                        epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    training_metrics = inference_model.compute_loss_metrics(metrics={_EVALUATION_METRIC.__name__: _EVALUATION_METRIC, 'mse': mse, 'r2': r2_score},\n",
    "                                                            X=XT_train, Y=YT_train)\n",
    "    print(training_metrics)\n",
    "\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_train[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_train[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_train[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### -Make prediction on 2020, the validation set, without computing equilibrium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=float('inf'),  # _RELATIVE_GAP,\n",
    "                            epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        # scatter_kws={\"color\": sns.color_palette(\"deep\")[0], 's':4, 'alpha': 1}, line_kws={\"color\": \"black\"},\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-without-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Make prediction on 2020, the validation set, computing equilibrium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "_ = inference_model.predict(XT_val,\n",
    "                            node_data=nodes_df,\n",
    "                            loss_metric=_LOSS_METRIC,\n",
    "                            batch_size= _BATCH_SIZE,\n",
    "                            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium']),\n",
    "                            pretrain_link_flows = False,\n",
    "                            loss_weights={'equilibrium': 1},\n",
    "                            threshold_relative_gap=_RELATIVE_GAP,\n",
    "                            epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "#     metrics_df = pd.concat([training_metrics.assign(dataset = 'training'),\n",
    "#                             validation_metrics.assign(dataset = 'validation')])\n",
    "#     print(pd.pivot(metrics_df, index=['component', 'dataset'], columns=['metric'])['value'])\n",
    "\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print('\\n')\n",
    "    validation_metrics = inference_model.compute_loss_metrics(metrics = {_EVALUATION_METRIC.__name__: _EVALUATION_METRIC, 'mse': mse, 'r2': r2_score}, X = XT_val, Y = YT_val)\n",
    "    print(validation_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_flow_vs_traveltime(model = inference_model,\n",
    "                        observed_traveltime=inference_model.mask_observed_traveltime(YT_val[:, :, 0]),\n",
    "                        observed_flow= inference_model.mask_observed_flow(YT_val[:,:,1]),\n",
    "                        period_col = pd.DataFrame({'period': list(XT_val[:, :, -1].numpy().astype(int).flatten())})['period'].map(dict(zip(period_keys.period_id, period_keys.hour))).values.flatten(),\n",
    "                        hour_label=True,\n",
    "                        all_metrics = False\n",
    "                        )\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.legend(loc='lower right', title = 'hour')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-scatter-flow-traveltime-outofsample-tvgodlulpe-with-equilibrium.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_kfold = {}\n",
    "metrics_kfold_df = {}\n",
    "parameters_kfold_df = {}\n",
    "_N_SPLITS = 5#10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Baselines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate centroids of links to compute baselines models\n",
    "links_gdf['X'] = links_gdf.to_crs(2228).geometry.centroid.to_crs(4326).x\n",
    "links_gdf['Y'] = links_gdf.to_crs(2228).geometry.centroid.to_crs(4326).y\n",
    "coordinates = links_gdf[['X', 'Y']].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baselines are computed for every sample\n",
    "metrics_kfold_df['baselines'] = pd.DataFrame({})\n",
    "n_samples = XT_val.shape[0]\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(n_samples):\n",
    "\n",
    "    isl.printer.printIterationBar(i+1, n_samples, prefix='Sample:',length=20)\n",
    "\n",
    "    metrics_kfold_df['baselines'] = pd.concat([metrics_kfold_df['baselines'],\n",
    "                                  compute_baseline_predictions_kfold(\n",
    "                                      metric=_EVALUATION_METRIC,\n",
    "                                      X = XT_val[i],\n",
    "                                      y = YT_val[i][:,0][tf.newaxis,:],\n",
    "                                      coordinates = coordinates,\n",
    "                                      n_splits = _N_SPLITS, seed = _SEED).assign(component = 'traveltime')])\n",
    "\n",
    "    metrics_kfold_df['baselines'] = pd.concat([metrics_kfold_df['baselines'],\n",
    "                                  compute_baseline_predictions_kfold(\n",
    "                                      metric=_EVALUATION_METRIC,\n",
    "                                      X = XT_val[i],\n",
    "                                      y = YT_val[i][:,1][tf.newaxis,:],\n",
    "                                      coordinates = coordinates,\n",
    "                                      n_splits = _N_SPLITS, seed = _SEED).assign(component = 'flow')])\n",
    "\n",
    "    metrics_kfold_df['baselines']['sample'] = i\n",
    "\n",
    "print(f'runtime: {time.time()-t0:0.1f} [s]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_df['baselines'].to_csv(f\"./output/experiments/{ts}_kfold_baselines_{network.key}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - TVODLULPE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_kfold['tvodlulpe'] = create_tvodlulpe_model_fresno(network = network, n_periods = n_periods, historic_q = q_historic,\n",
    "                                                    features_Z = _FEATURES_Z, dtype = _DTYPE)\n",
    "models_kfold['tvodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "#models_kfold['tvodlulpe'].generation._pretrain_generation_weights = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Use pretrained weights\n",
    "models_kfold['tvodlulpe'].load_weights(model_filepaths['tvodlulpe'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "q_reference = models_kfold['tvodlulpe'].q\n",
    "\n",
    "q_reference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvodlulpe'].od.historic_values = q_reference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'], parameters_kfold_df['tvodlulpe'] = train_kfold(\n",
    "    n_splits=_N_SPLITS,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE['tvodlulpe'],\n",
    "    epochs_print_interval= _EPOCHS['tvodlulpe'],\n",
    "    pretrain_link_flows = True,\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    epochs = _EPOCHS['tvodlulpe'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvodlulpe'].to_csv(f\"./output/experiments/{ts}_kfold_{network.key}.csv\")\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvodlulpe'][metrics_kfold_df['tvodlulpe'].component.isin(['flow','traveltime'])],\n",
    "                              metric_name = _EVALUATION_METRIC.__name__, model_name = 'tvodlulpe', showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig(f'output/figures/results/fresno-kfold-{_EVALUATION_METRIC.__name__}-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvodlulpe']['hour'] = parameters_kfold_df['tvodlulpe'].period.replace({v: k for k, v in models_kfold['tvodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['parameter'] = parameters_kfold_df['tvodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvodlulpe'] = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvodlulpe'][parameters_kfold_df['tvodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvodlulpe.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - TVGODLULPE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_periods = len(np.unique(XT_train[:, :, -1].numpy().flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_kfold['tvgodlulpe'] = create_tvgodlulpe_model_fresno(network = network, n_periods = n_periods, features_Z = _FEATURES_Z,\n",
    "                                                               historic_g = generated_trips, historic_q = q_historic)\n",
    "\n",
    "models_kfold['tvgodlulpe'].build()\n",
    "\n",
    "# Prevent to repretrain generation weights\n",
    "models_kfold['tvgodlulpe'].generation._pretrain_generation_weights = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use pretrained weights\n",
    "models_kfold['tvgodlulpe'].load_weights(model_filepaths['tvgodlulpe'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assumed there is access to reference OD matrix\n",
    "q_reference = models_kfold['tvgodlulpe'].q\n",
    "\n",
    "q_reference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add historic OD from estimation in 2019\n",
    "models_kfold['tvgodlulpe'].od.historic_values = q_reference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add deviation respect to historic OD matrix for better generalization.\n",
    "_LOSS_WEIGHTS_KFOLD ={'od': 1, 'traveltime': 1, 'flow': 1, 'equilibrium': 1}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_df['tvgodlulpe'], parameters_kfold_df['tvgodlulpe'] = train_kfold(\n",
    "    n_splits=_N_SPLITS ,\n",
    "    random_state = _SEED,\n",
    "    model = models_kfold['tvgodlulpe'],\n",
    "    X = XT_val, Y = YT_val,\n",
    "    optimizers= {'learning': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['learning']),\n",
    "                 'equilibrium': tf.keras.optimizers.legacy.Adam(learning_rate=_LR['equilibrium'])\n",
    "                 },\n",
    "    node_data = nodes_df,\n",
    "    loss_weights=_LOSS_WEIGHTS_KFOLD,\n",
    "    loss_metric=_LOSS_METRIC,\n",
    "    evaluation_metric=_EVALUATION_METRIC,\n",
    "    equilibrium_stage=_EQUILIBRIUM_STAGE['tvgodlulpe'],\n",
    "    pretrain_link_flows = True,\n",
    "    epochs_print_interval= _EPOCHS['tvgodlulpe'],\n",
    "    threshold_relative_gap= _RELATIVE_GAP,\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    epochs = _EPOCHS['tvgodlulpe'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = f\"{ts}_kfold_{network.key}.csv\"\n",
    "metrics_kfold_df['tvgodlulpe'].to_csv(f\"./output/experiments/{filename}\")\n",
    "print(f\"File {filename} written with k-fold results for TVGODULPE model\")\n",
    "\n",
    "\n",
    "# TODO: Add coefficient of variation and save experiments results, compute percentage reduction between final and initial\n",
    "with pd.option_context('display.float_format', '{:0.3g}'.format):\n",
    "    print(metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])].\\\n",
    "          groupby(['dataset', 'component', 'metric', 'stage'])['value'].\\\n",
    "          aggregate(['median', 'mean', 'std']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(metrics_kfold_df['tvgodlulpe']['stage'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plot_metrics_kfold(df = metrics_kfold_df['tvgodlulpe'][metrics_kfold_df['tvgodlulpe'].component.isin(['flow','traveltime'])], model_name = 'tvgodlulpe', metric_name = _EVALUATION_METRIC.__name__, showfliers = True)\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom = 0)\n",
    "\n",
    "plt.savefig(f'output/figures/results/fresno-kfold-{_EVALUATION_METRIC.__name__}-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters_kfold_df['tvgodlulpe']['hour'] = parameters_kfold_df['tvgodlulpe'].period.replace({v: k for k, v in models_kfold['tvgodlulpe'].period_dict.items()}). \\\n",
    "    replace(dict(zip(period_keys.period_id, period_keys.hour)))\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['parameter'] = parameters_kfold_df['tvgodlulpe']['parameter'].\\\n",
    "    replace({'tt': 'travel time', 'median_inc': 'income', 'tt_sd': 'std. travel time', 'bus_stops': 'bus stops', 'vot': 'reliability ratio'})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe']['hour'] \\\n",
    "    = parameters_kfold_df['tvgodlulpe']['period'].map({v:period_keys[period_keys.period_id == k]['hour'].iloc[0] for k,v in models_kfold['tvgodlulpe'].period_dict.items()})\n",
    "\n",
    "parameters_kfold_df['tvgodlulpe'] = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].parameter != 'reliability ratio']\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'utility'], n_cols_legend = 2, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-utility-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plot_parameters_kfold(df = parameters_kfold_df['tvgodlulpe'][parameters_kfold_df['tvgodlulpe'].group == 'generation'], n_cols_legend = 3, figsize = (5.5,5.5), hour_label = True)\n",
    "\n",
    "plt.axhline(0, linestyle ='dashed', color = 'black')\n",
    "\n",
    "plt.savefig('output/figures/results/fresno-kfold-generation-periods-tvgodlulpe.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison between our model and benchmarks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_plot_df = metrics_kfold_df['tvgodlulpe'][\n",
    "        (metrics_kfold_df['tvgodlulpe'].component != 'equilibrium') & (metrics_kfold_df['tvgodlulpe'].metric == _EVALUATION_METRIC.__name__) &\n",
    "        (metrics_kfold_df['tvgodlulpe'].dataset == 'validation') & (metrics_kfold_df['tvgodlulpe'].stage == 'final')].\\\n",
    "        assign(model = 'tvgodlulpe')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_kfold_df['baselines'] \\\n",
    "    = metrics_kfold_df['baselines'].groupby(['fold', 'model', 'component'])[['value']].mean().reset_index()\n",
    "\n",
    "metrics_kfold_plot_df = pd.concat([metrics_kfold_df['baselines'][['model','component','value']],\n",
    "                                   metrics_kfold_plot_df[['model','component','value']]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means_mapes = metrics_kfold_plot_df.groupby(['component','model'])['value'].mean().reset_index().rename(columns={'value':'mean'})\n",
    "se_mapes = metrics_kfold_plot_df.groupby(['component','model'])['value'].agg(['std', 'count']).apply(lambda row: row['std'] / np.sqrt(row['count']), axis=1).reset_index().rename(columns={0:'se'})\n",
    "\n",
    "pd.merge(means_mapes, se_mapes).sort_values(by = ['component', 'mean'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note: Result from ordinary kriging are removed due to unstable estimates and extremely large values of evaluation metric for travel time\n",
    "plot_baselines_kfold(df = metrics_kfold_plot_df[metrics_kfold_plot_df.model != 'ordinary_kriging'],\n",
    "                     metric_name=_EVALUATION_METRIC.__name__, sharex=True, sharey=True, showfliers = False)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Link-level spatial information\n",
    "links_gdf['link_key'] = pd.Categorical(links_gdf['key'].apply(ast.literal_eval), list(network.links_dict.keys()))\n",
    "\n",
    "# Create dataframe with data collected in 2020 during peak hours only\n",
    "model_df = df[(df.hour.isin([6,7,8, 15,16,17])) & (df['year']==2020)].sort_values(['period','link_key'])\n",
    "# links_gdf = links_gdf.sort_values(['link_key'])\n",
    "\n",
    "# Build dataset witg data collected between 4-5pm in the first Tuesdays of Oct 2019 and 2020\n",
    "benchmark_df = df[(df.hour == 16) & df['date'].isin(['2019-10-01', '2020-10-06'])].sort_values(['period','link_key'])\n",
    "\n",
    "fig_speed, fig_flow = plot_congestion_maps(model=inference_model, model_df=model_df, benchmark_df = benchmark_df,\n",
    "                     gdf=links_gdf.sort_values(['link_key']), features=_FEATURES_Z, cmap = 'viridis')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_flow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_speed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Global runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'runtime: {time.time()-t0_global:0.1f} [s]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nesuelogit",
   "language": "python",
   "name": "nesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
